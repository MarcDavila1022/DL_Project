{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrQU-SHEuujG",
        "outputId": "69dc7556-ffbb-4b6e-99e0-bfeff97eef69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, torch-fidelity\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-fidelity-0.3.0 torchmetrics-1.7.1\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.3.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchmetrics torch-fidelity numpy matplotlib scikit-image\n",
        "!pip install pytorch-fid  # For FID calculation\n",
        "!pip install tqdm\n",
        "# Install required packages\n",
        "!pip install -q kornia tqdm einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqTEe3i-ur_Q",
        "outputId": "1a2c0f8c-0be0-4486-a75f-18d37cc48a2a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "Is Time embed used ?  True\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.5MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/Project/cifar10/model_110000.pt\n",
            "Starting training for 50 epochs (200000 steps)...\n",
            "Step 110000: Loss = 0.055589\n",
            "Step 110000: Loss = 0.051443\n",
            "\n",
            "Saving samples at step 110000\n",
            "Average loss over last 1000 steps: 0.000054\n",
            "Model saved at step 110000\n",
            "\n",
            "Evaluating model at step 110000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100%|██████████| 91.2M/91.2M [00:02<00:00, 42.4MB/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.42it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.24it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.49it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.82it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 110000:\n",
            "Direct reconstruction: SSIM=0.8545, RMSE=0.0966, FID=184.41\n",
            "Deblurred (Algorithm 2): SSIM=0.7787, RMSE=0.1327, FID=176.01\n",
            "Step 110100: Loss = 0.040513\n",
            "Step 110100: Loss = 0.050713\n",
            "Step 110200: Loss = 0.053033\n",
            "Step 110200: Loss = 0.054532\n",
            "Step 110300: Loss = 0.052247\n",
            "Step 110300: Loss = 0.058440\n",
            "Step 110400: Loss = 0.047899\n",
            "Step 110400: Loss = 0.052425\n",
            "Step 110500: Loss = 0.056654\n",
            "Step 110500: Loss = 0.057279\n",
            "Step 110600: Loss = 0.053105\n",
            "Step 110600: Loss = 0.051403\n",
            "Step 110700: Loss = 0.055127\n",
            "Step 110700: Loss = 0.053607\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 110800: Loss = 0.051494\n",
            "Step 110800: Loss = 0.049341\n",
            "Step 110900: Loss = 0.050152\n",
            "Step 110900: Loss = 0.058397\n",
            "Step 111000: Loss = 0.056159\n",
            "Step 111000: Loss = 0.052885\n",
            "\n",
            "Saving samples at step 111000\n",
            "Average loss over last 1000 steps: 0.053480\n",
            "Step 111100: Loss = 0.048697\n",
            "Step 111100: Loss = 0.053227\n",
            "Step 111200: Loss = 0.052164\n",
            "Step 111200: Loss = 0.050322\n",
            "Step 111300: Loss = 0.054191\n",
            "Step 111300: Loss = 0.050957\n",
            "Step 111400: Loss = 0.054332\n",
            "Step 111400: Loss = 0.049473\n",
            "Step 111500: Loss = 0.047252\n",
            "Step 111500: Loss = 0.051949\n",
            "Step 111600: Loss = 0.042478\n",
            "Step 111600: Loss = 0.060494\n",
            "Step 111700: Loss = 0.054231\n",
            "Step 111700: Loss = 0.054048\n",
            "Step 111800: Loss = 0.046805\n",
            "Step 111800: Loss = 0.052990\n",
            "Step 111900: Loss = 0.049807\n",
            "Step 111900: Loss = 0.051549\n",
            "Step 112000: Loss = 0.060223\n",
            "Step 112000: Loss = 0.052049\n",
            "\n",
            "Saving samples at step 112000\n",
            "Average loss over last 1000 steps: 0.053256\n",
            "Step 112100: Loss = 0.052444\n",
            "Step 112100: Loss = 0.057199\n",
            "Step 112200: Loss = 0.057390\n",
            "Step 112200: Loss = 0.056935\n",
            "Step 112300: Loss = 0.044624\n",
            "Step 112300: Loss = 0.052391\n",
            "Step 112400: Loss = 0.053238\n",
            "Step 112400: Loss = 0.052050\n",
            "Step 112500: Loss = 0.045396\n",
            "Step 112500: Loss = 0.052569\n",
            "Step 112600: Loss = 0.053362\n",
            "Step 112600: Loss = 0.056982\n",
            "Step 112700: Loss = 0.060528\n",
            "Step 112700: Loss = 0.052817\n",
            "Step 112800: Loss = 0.050856\n",
            "Step 112800: Loss = 0.051029\n",
            "Step 112900: Loss = 0.049937\n",
            "Step 112900: Loss = 0.052104\n",
            "Step 113000: Loss = 0.052858\n",
            "Step 113000: Loss = 0.056190\n",
            "\n",
            "Saving samples at step 113000\n",
            "Average loss over last 1000 steps: 0.052943\n",
            "Step 113100: Loss = 0.053262\n",
            "Step 113100: Loss = 0.056540\n",
            "Step 113200: Loss = 0.057463\n",
            "Step 113200: Loss = 0.047324\n",
            "Step 113300: Loss = 0.048564\n",
            "Step 113300: Loss = 0.056080\n",
            "Step 113400: Loss = 0.056850\n",
            "Step 113400: Loss = 0.053144\n",
            "Step 113500: Loss = 0.051116\n",
            "Step 113500: Loss = 0.051959\n",
            "Step 113600: Loss = 0.051711\n",
            "Step 113600: Loss = 0.053352\n",
            "Step 113700: Loss = 0.052807\n",
            "Step 113700: Loss = 0.049214\n",
            "Step 113800: Loss = 0.054415\n",
            "Step 113800: Loss = 0.050636\n",
            "Step 113900: Loss = 0.050886\n",
            "Step 113900: Loss = 0.050915\n",
            "Step 114000: Loss = 0.049513\n",
            "Step 114000: Loss = 0.053873\n",
            "\n",
            "Saving samples at step 114000\n",
            "Average loss over last 1000 steps: 0.052572\n",
            "Step 114100: Loss = 0.053211\n",
            "Step 114100: Loss = 0.053430\n",
            "Step 114200: Loss = 0.050142\n",
            "Step 114200: Loss = 0.057708\n",
            "Step 114300: Loss = 0.047522\n",
            "Step 114300: Loss = 0.038225\n",
            "Step 114400: Loss = 0.054341\n",
            "Step 114400: Loss = 0.053691\n",
            "Step 114500: Loss = 0.050762\n",
            "Step 114500: Loss = 0.055941\n",
            "Step 114600: Loss = 0.052702\n",
            "Step 114600: Loss = 0.046304\n",
            "Step 114700: Loss = 0.042902\n",
            "Step 114700: Loss = 0.046056\n",
            "Step 114800: Loss = 0.049770\n",
            "Step 114800: Loss = 0.049978\n",
            "Step 114900: Loss = 0.057754\n",
            "Step 114900: Loss = 0.052436\n",
            "Step 115000: Loss = 0.058963\n",
            "Step 115000: Loss = 0.053075\n",
            "\n",
            "Saving samples at step 115000\n",
            "Average loss over last 1000 steps: 0.052469\n",
            "Step 115100: Loss = 0.046674\n",
            "Step 115100: Loss = 0.048902\n",
            "Step 115200: Loss = 0.050959\n",
            "Step 115200: Loss = 0.051162\n",
            "Step 115300: Loss = 0.049259\n",
            "Step 115300: Loss = 0.048053\n",
            "Step 115400: Loss = 0.057076\n",
            "Step 115400: Loss = 0.046343\n",
            "Step 115500: Loss = 0.048799\n",
            "Step 115500: Loss = 0.052479\n",
            "Step 115600: Loss = 0.047761\n",
            "Step 115600: Loss = 0.055851\n",
            "Step 115700: Loss = 0.055473\n",
            "Step 115700: Loss = 0.056456\n",
            "Step 115800: Loss = 0.053044\n",
            "Step 115800: Loss = 0.057619\n",
            "Step 115900: Loss = 0.048164\n",
            "Step 115900: Loss = 0.045312\n",
            "Step 116000: Loss = 0.059531\n",
            "Step 116000: Loss = 0.053871\n",
            "\n",
            "Saving samples at step 116000\n",
            "Average loss over last 1000 steps: 0.052092\n",
            "Step 116100: Loss = 0.064221\n",
            "Step 116100: Loss = 0.045124\n",
            "Step 116200: Loss = 0.047723\n",
            "Step 116200: Loss = 0.059170\n",
            "Step 116300: Loss = 0.050406\n",
            "Step 116300: Loss = 0.041936\n",
            "Step 116400: Loss = 0.056249\n",
            "Step 116400: Loss = 0.056238\n",
            "Step 116500: Loss = 0.055781\n",
            "Step 116500: Loss = 0.050823\n",
            "Step 116600: Loss = 0.045387\n",
            "Step 116600: Loss = 0.053161\n",
            "Step 116700: Loss = 0.059030\n",
            "Step 116700: Loss = 0.057329\n",
            "Step 116800: Loss = 0.048530\n",
            "Step 116800: Loss = 0.054583\n",
            "Step 116900: Loss = 0.050592\n",
            "Step 116900: Loss = 0.053135\n",
            "Step 117000: Loss = 0.048077\n",
            "Step 117000: Loss = 0.047584\n",
            "\n",
            "Saving samples at step 117000\n",
            "Average loss over last 1000 steps: 0.052008\n",
            "Step 117100: Loss = 0.057596\n",
            "Step 117100: Loss = 0.047820\n",
            "Step 117200: Loss = 0.047081\n",
            "Step 117200: Loss = 0.054652\n",
            "Step 117300: Loss = 0.052705\n",
            "Step 117300: Loss = 0.055471\n",
            "Step 117400: Loss = 0.053846\n",
            "Step 117400: Loss = 0.047657\n",
            "Step 117500: Loss = 0.047170\n",
            "Step 117500: Loss = 0.044776\n",
            "Step 117600: Loss = 0.052001\n",
            "Step 117600: Loss = 0.051847\n",
            "Step 117700: Loss = 0.043931\n",
            "Step 117700: Loss = 0.056502\n",
            "Step 117800: Loss = 0.054064\n",
            "Step 117800: Loss = 0.054782\n",
            "Step 117900: Loss = 0.055186\n",
            "Step 117900: Loss = 0.058539\n",
            "Step 118000: Loss = 0.050755\n",
            "Step 118000: Loss = 0.047791\n",
            "\n",
            "Saving samples at step 118000\n",
            "Average loss over last 1000 steps: 0.051659\n",
            "Step 118100: Loss = 0.058947\n",
            "Step 118100: Loss = 0.049211\n",
            "Step 118200: Loss = 0.054812\n",
            "Step 118200: Loss = 0.054748\n",
            "Step 118300: Loss = 0.050654\n",
            "Step 118300: Loss = 0.042371\n",
            "Step 118400: Loss = 0.052372\n",
            "Step 118400: Loss = 0.053806\n",
            "Step 118500: Loss = 0.052704\n",
            "Step 118500: Loss = 0.053710\n",
            "Step 118600: Loss = 0.053531\n",
            "Step 118600: Loss = 0.048126\n",
            "Step 118700: Loss = 0.055395\n",
            "Step 118700: Loss = 0.054052\n",
            "Step 118800: Loss = 0.055519\n",
            "Step 118800: Loss = 0.052597\n",
            "Step 118900: Loss = 0.051038\n",
            "Step 118900: Loss = 0.054385\n",
            "Step 119000: Loss = 0.051724\n",
            "Step 119000: Loss = 0.043548\n",
            "\n",
            "Saving samples at step 119000\n",
            "Average loss over last 1000 steps: 0.051635\n",
            "Step 119100: Loss = 0.050883\n",
            "Step 119100: Loss = 0.043875\n",
            "Step 119200: Loss = 0.048381\n",
            "Step 119200: Loss = 0.052972\n",
            "Step 119300: Loss = 0.044191\n",
            "Step 119300: Loss = 0.056528\n",
            "Step 119400: Loss = 0.047644\n",
            "Step 119400: Loss = 0.043979\n",
            "Step 119500: Loss = 0.055115\n",
            "Step 119500: Loss = 0.051182\n",
            "Step 119600: Loss = 0.042908\n",
            "Step 119600: Loss = 0.048049\n",
            "Step 119700: Loss = 0.051593\n",
            "Step 119700: Loss = 0.052389\n",
            "Step 119800: Loss = 0.056471\n",
            "Step 119800: Loss = 0.040488\n",
            "Step 119900: Loss = 0.046480\n",
            "Step 119900: Loss = 0.046583\n",
            "Step 120000: Loss = 0.053105\n",
            "Step 120000: Loss = 0.048090\n",
            "\n",
            "Saving samples at step 120000\n",
            "Average loss over last 1000 steps: 0.051130\n",
            "Model saved at step 120000\n",
            "\n",
            "Evaluating model at step 120000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  4.01it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.07it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.77it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 120000:\n",
            "Direct reconstruction: SSIM=0.8695, RMSE=0.0906, FID=176.31\n",
            "Deblurred (Algorithm 2): SSIM=0.7980, RMSE=0.1258, FID=170.33\n",
            "Step 120100: Loss = 0.047140\n",
            "Step 120100: Loss = 0.042617\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 120200: Loss = 0.051119\n",
            "Step 120200: Loss = 0.050028\n",
            "Step 120300: Loss = 0.060766\n",
            "Step 120300: Loss = 0.055664\n",
            "Step 120400: Loss = 0.048456\n",
            "Step 120400: Loss = 0.050442\n",
            "Step 120500: Loss = 0.058009\n",
            "Step 120500: Loss = 0.048300\n",
            "Step 120600: Loss = 0.056807\n",
            "Step 120600: Loss = 0.053776\n",
            "Step 120700: Loss = 0.051912\n",
            "Step 120700: Loss = 0.056474\n",
            "Step 120800: Loss = 0.047658\n",
            "Step 120800: Loss = 0.047239\n",
            "Step 120900: Loss = 0.046451\n",
            "Step 120900: Loss = 0.051546\n",
            "Step 121000: Loss = 0.046599\n",
            "Step 121000: Loss = 0.047136\n",
            "\n",
            "Saving samples at step 121000\n",
            "Average loss over last 1000 steps: 0.051100\n",
            "Step 121100: Loss = 0.049037\n",
            "Step 121100: Loss = 0.046721\n",
            "Step 121200: Loss = 0.052056\n",
            "Step 121200: Loss = 0.050697\n",
            "Step 121300: Loss = 0.050662\n",
            "Step 121300: Loss = 0.056075\n",
            "Step 121400: Loss = 0.047638\n",
            "Step 121400: Loss = 0.042601\n",
            "Step 121500: Loss = 0.051191\n",
            "Step 121500: Loss = 0.042312\n",
            "Step 121600: Loss = 0.050919\n",
            "Step 121600: Loss = 0.059549\n",
            "Step 121700: Loss = 0.054360\n",
            "Step 121700: Loss = 0.048039\n",
            "Step 121800: Loss = 0.049394\n",
            "Step 121800: Loss = 0.055744\n",
            "Step 121900: Loss = 0.053604\n",
            "Step 121900: Loss = 0.049348\n",
            "Step 122000: Loss = 0.052349\n",
            "Step 122000: Loss = 0.054596\n",
            "\n",
            "Saving samples at step 122000\n",
            "Average loss over last 1000 steps: 0.050931\n",
            "Step 122100: Loss = 0.052417\n",
            "Step 122100: Loss = 0.053423\n",
            "Step 122200: Loss = 0.056438\n",
            "Step 122200: Loss = 0.051811\n",
            "Step 122300: Loss = 0.056362\n",
            "Step 122300: Loss = 0.053169\n",
            "Step 122400: Loss = 0.051451\n",
            "Step 122400: Loss = 0.049470\n",
            "Step 122500: Loss = 0.042815\n",
            "Step 122500: Loss = 0.050983\n",
            "Step 122600: Loss = 0.049665\n",
            "Step 122600: Loss = 0.052898\n",
            "Step 122700: Loss = 0.046039\n",
            "Step 122700: Loss = 0.054361\n",
            "Step 122800: Loss = 0.059084\n",
            "Step 122800: Loss = 0.048913\n",
            "Step 122900: Loss = 0.055373\n",
            "Step 122900: Loss = 0.051872\n",
            "Step 123000: Loss = 0.046759\n",
            "Step 123000: Loss = 0.056139\n",
            "\n",
            "Saving samples at step 123000\n",
            "Average loss over last 1000 steps: 0.050550\n",
            "Step 123100: Loss = 0.050155\n",
            "Step 123100: Loss = 0.051604\n",
            "Step 123200: Loss = 0.044638\n",
            "Step 123200: Loss = 0.039923\n",
            "Step 123300: Loss = 0.053754\n",
            "Step 123300: Loss = 0.044775\n",
            "Step 123400: Loss = 0.049538\n",
            "Step 123400: Loss = 0.050176\n",
            "Step 123500: Loss = 0.049732\n",
            "Step 123500: Loss = 0.045590\n",
            "Step 123600: Loss = 0.053886\n",
            "Step 123600: Loss = 0.053601\n",
            "Step 123700: Loss = 0.047208\n",
            "Step 123700: Loss = 0.045946\n",
            "Step 123800: Loss = 0.045636\n",
            "Step 123800: Loss = 0.042857\n",
            "Step 123900: Loss = 0.052591\n",
            "Step 123900: Loss = 0.048823\n",
            "Step 124000: Loss = 0.054203\n",
            "Step 124000: Loss = 0.047712\n",
            "\n",
            "Saving samples at step 124000\n",
            "Average loss over last 1000 steps: 0.050362\n",
            "Step 124100: Loss = 0.052221\n",
            "Step 124100: Loss = 0.047093\n",
            "Step 124200: Loss = 0.047721\n",
            "Step 124200: Loss = 0.056931\n",
            "Step 124300: Loss = 0.049955\n",
            "Step 124300: Loss = 0.053926\n",
            "Step 124400: Loss = 0.055038\n",
            "Step 124400: Loss = 0.050595\n",
            "Step 124500: Loss = 0.044706\n",
            "Step 124500: Loss = 0.055376\n",
            "Step 124600: Loss = 0.049408\n",
            "Step 124600: Loss = 0.047997\n",
            "Step 124700: Loss = 0.046616\n",
            "Step 124700: Loss = 0.049082\n",
            "Step 124800: Loss = 0.054214\n",
            "Step 124800: Loss = 0.055125\n",
            "Step 124900: Loss = 0.045590\n",
            "Step 124900: Loss = 0.048975\n",
            "Step 125000: Loss = 0.053428\n",
            "Step 125000: Loss = 0.054255\n",
            "\n",
            "Saving samples at step 125000\n",
            "Average loss over last 1000 steps: 0.050273\n",
            "Step 125100: Loss = 0.045512\n",
            "Step 125100: Loss = 0.050375\n",
            "Step 125200: Loss = 0.053964\n",
            "Step 125200: Loss = 0.053422\n",
            "Step 125300: Loss = 0.050135\n",
            "Step 125300: Loss = 0.053142\n",
            "Step 125400: Loss = 0.050062\n",
            "Step 125400: Loss = 0.052629\n",
            "Step 125500: Loss = 0.046073\n",
            "Step 125500: Loss = 0.047718\n",
            "Step 125600: Loss = 0.051335\n",
            "Step 125600: Loss = 0.050631\n",
            "Step 125700: Loss = 0.048731\n",
            "Step 125700: Loss = 0.050525\n",
            "Step 125800: Loss = 0.049789\n",
            "Step 125800: Loss = 0.054974\n",
            "Step 125900: Loss = 0.052951\n",
            "Step 125900: Loss = 0.046338\n",
            "Step 126000: Loss = 0.044813\n",
            "Step 126000: Loss = 0.046679\n",
            "\n",
            "Saving samples at step 126000\n",
            "Average loss over last 1000 steps: 0.049900\n",
            "Step 126100: Loss = 0.052000\n",
            "Step 126100: Loss = 0.055806\n",
            "Step 126200: Loss = 0.049055\n",
            "Step 126200: Loss = 0.055904\n",
            "Step 126300: Loss = 0.050226\n",
            "Step 126300: Loss = 0.050663\n",
            "Step 126400: Loss = 0.053368\n",
            "Step 126400: Loss = 0.047929\n",
            "Step 126500: Loss = 0.046947\n",
            "Step 126500: Loss = 0.053142\n",
            "Step 126600: Loss = 0.049926\n",
            "Step 126600: Loss = 0.053183\n",
            "Step 126700: Loss = 0.044967\n",
            "Step 126700: Loss = 0.055133\n",
            "Step 126800: Loss = 0.046678\n",
            "Step 126800: Loss = 0.051300\n",
            "Step 126900: Loss = 0.049567\n",
            "Step 126900: Loss = 0.045052\n",
            "Step 127000: Loss = 0.040168\n",
            "Step 127000: Loss = 0.042914\n",
            "\n",
            "Saving samples at step 127000\n",
            "Average loss over last 1000 steps: 0.049771\n",
            "Step 127100: Loss = 0.050335\n",
            "Step 127100: Loss = 0.044054\n",
            "Step 127200: Loss = 0.051099\n",
            "Step 127200: Loss = 0.045271\n",
            "Step 127300: Loss = 0.049941\n",
            "Step 127300: Loss = 0.049892\n",
            "Step 127400: Loss = 0.049450\n",
            "Step 127400: Loss = 0.046483\n",
            "Step 127500: Loss = 0.050625\n",
            "Step 127500: Loss = 0.049254\n",
            "Step 127600: Loss = 0.049047\n",
            "Step 127600: Loss = 0.050880\n",
            "Step 127700: Loss = 0.059017\n",
            "Step 127700: Loss = 0.054753\n",
            "Step 127800: Loss = 0.043719\n",
            "Step 127800: Loss = 0.049020\n",
            "Step 127900: Loss = 0.053336\n",
            "Step 127900: Loss = 0.045383\n",
            "Step 128000: Loss = 0.051457\n",
            "Step 128000: Loss = 0.055293\n",
            "\n",
            "Saving samples at step 128000\n",
            "Average loss over last 1000 steps: 0.049641\n",
            "Step 128100: Loss = 0.048593\n",
            "Step 128100: Loss = 0.048510\n",
            "Step 128200: Loss = 0.044571\n",
            "Step 128200: Loss = 0.050166\n",
            "Step 128300: Loss = 0.046056\n",
            "Step 128300: Loss = 0.051149\n",
            "Step 128400: Loss = 0.044444\n",
            "Step 128400: Loss = 0.046706\n",
            "Step 128500: Loss = 0.051267\n",
            "Step 128500: Loss = 0.046458\n",
            "Step 128600: Loss = 0.047846\n",
            "Step 128600: Loss = 0.046158\n",
            "Step 128700: Loss = 0.046231\n",
            "Step 128700: Loss = 0.057712\n",
            "Step 128800: Loss = 0.056153\n",
            "Step 128800: Loss = 0.051571\n",
            "Step 128900: Loss = 0.041678\n",
            "Step 128900: Loss = 0.049361\n",
            "Step 129000: Loss = 0.048226\n",
            "Step 129000: Loss = 0.051714\n",
            "\n",
            "Saving samples at step 129000\n",
            "Average loss over last 1000 steps: 0.049353\n",
            "Step 129100: Loss = 0.050875\n",
            "Step 129100: Loss = 0.045820\n",
            "Step 129200: Loss = 0.043727\n",
            "Step 129200: Loss = 0.057035\n",
            "Step 129300: Loss = 0.049671\n",
            "Step 129300: Loss = 0.054377\n",
            "Step 129400: Loss = 0.041368\n",
            "Step 129400: Loss = 0.049676\n",
            "Step 129500: Loss = 0.045725\n",
            "Step 129500: Loss = 0.049580\n",
            "Step 129600: Loss = 0.043830\n",
            "Step 129600: Loss = 0.047804\n",
            "Step 129700: Loss = 0.043308\n",
            "Step 129700: Loss = 0.044088\n",
            "Step 129800: Loss = 0.043938\n",
            "Step 129800: Loss = 0.056179\n",
            "Step 129900: Loss = 0.049739\n",
            "Step 129900: Loss = 0.050373\n",
            "Step 130000: Loss = 0.050260\n",
            "Step 130000: Loss = 0.043741\n",
            "\n",
            "Saving samples at step 130000\n",
            "Average loss over last 1000 steps: 0.049215\n",
            "Model saved at step 130000\n",
            "\n",
            "Evaluating model at step 130000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.97it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 130000:\n",
            "Direct reconstruction: SSIM=0.8804, RMSE=0.0863, FID=172.75\n",
            "Deblurred (Algorithm 2): SSIM=0.8096, RMSE=0.1223, FID=167.86\n",
            "Step 130100: Loss = 0.046845\n",
            "Step 130100: Loss = 0.051682\n",
            "Step 130200: Loss = 0.048954\n",
            "Step 130200: Loss = 0.048567\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 130300: Loss = 0.054585\n",
            "Step 130300: Loss = 0.049101\n",
            "Step 130400: Loss = 0.046736\n",
            "Step 130400: Loss = 0.047048\n",
            "Step 130500: Loss = 0.046210\n",
            "Step 130500: Loss = 0.051135\n",
            "Step 130600: Loss = 0.052190\n",
            "Step 130600: Loss = 0.048168\n",
            "Step 130700: Loss = 0.047618\n",
            "Step 130700: Loss = 0.051613\n",
            "Step 130800: Loss = 0.045105\n",
            "Step 130800: Loss = 0.042674\n",
            "Step 130900: Loss = 0.047669\n",
            "Step 130900: Loss = 0.046136\n",
            "Step 131000: Loss = 0.049667\n",
            "Step 131000: Loss = 0.042221\n",
            "\n",
            "Saving samples at step 131000\n",
            "Average loss over last 1000 steps: 0.049045\n",
            "Step 131100: Loss = 0.050448\n",
            "Step 131100: Loss = 0.044364\n",
            "Step 131200: Loss = 0.045550\n",
            "Step 131200: Loss = 0.046763\n",
            "Step 131300: Loss = 0.046285\n",
            "Step 131300: Loss = 0.046314\n",
            "Step 131400: Loss = 0.048048\n",
            "Step 131400: Loss = 0.048156\n",
            "Step 131500: Loss = 0.052495\n",
            "Step 131500: Loss = 0.046544\n",
            "Step 131600: Loss = 0.045919\n",
            "Step 131600: Loss = 0.052020\n",
            "Step 131700: Loss = 0.043792\n",
            "Step 131700: Loss = 0.047986\n",
            "Step 131800: Loss = 0.050925\n",
            "Step 131800: Loss = 0.052298\n",
            "Step 131900: Loss = 0.050613\n",
            "Step 131900: Loss = 0.048791\n",
            "Step 132000: Loss = 0.047710\n",
            "Step 132000: Loss = 0.054496\n",
            "\n",
            "Saving samples at step 132000\n",
            "Average loss over last 1000 steps: 0.048630\n",
            "Step 132100: Loss = 0.047952\n",
            "Step 132100: Loss = 0.042827\n",
            "Step 132200: Loss = 0.050368\n",
            "Step 132200: Loss = 0.045790\n",
            "Step 132300: Loss = 0.043847\n",
            "Step 132300: Loss = 0.057195\n",
            "Step 132400: Loss = 0.052650\n",
            "Step 132400: Loss = 0.040229\n",
            "Step 132500: Loss = 0.053303\n",
            "Step 132500: Loss = 0.051449\n",
            "Step 132600: Loss = 0.042766\n",
            "Step 132600: Loss = 0.041992\n",
            "Step 132700: Loss = 0.051173\n",
            "Step 132700: Loss = 0.042904\n",
            "Step 132800: Loss = 0.046562\n",
            "Step 132800: Loss = 0.048486\n",
            "Step 132900: Loss = 0.052914\n",
            "Step 132900: Loss = 0.052171\n",
            "Step 133000: Loss = 0.052857\n",
            "Step 133000: Loss = 0.048825\n",
            "\n",
            "Saving samples at step 133000\n",
            "Average loss over last 1000 steps: 0.048715\n",
            "Step 133100: Loss = 0.042674\n",
            "Step 133100: Loss = 0.049786\n",
            "Step 133200: Loss = 0.041224\n",
            "Step 133200: Loss = 0.048148\n",
            "Step 133300: Loss = 0.047935\n",
            "Step 133300: Loss = 0.052484\n",
            "Step 133400: Loss = 0.054067\n",
            "Step 133400: Loss = 0.049897\n",
            "Step 133500: Loss = 0.050558\n",
            "Step 133500: Loss = 0.056215\n",
            "Step 133600: Loss = 0.048586\n",
            "Step 133600: Loss = 0.050016\n",
            "Step 133700: Loss = 0.050935\n",
            "Step 133700: Loss = 0.043967\n",
            "Step 133800: Loss = 0.050008\n",
            "Step 133800: Loss = 0.050239\n",
            "Step 133900: Loss = 0.052138\n",
            "Step 133900: Loss = 0.046924\n",
            "Step 134000: Loss = 0.045202\n",
            "Step 134000: Loss = 0.040981\n",
            "\n",
            "Saving samples at step 134000\n",
            "Average loss over last 1000 steps: 0.048389\n",
            "Step 134100: Loss = 0.042781\n",
            "Step 134100: Loss = 0.049222\n",
            "Step 134200: Loss = 0.049044\n",
            "Step 134200: Loss = 0.047280\n",
            "Step 134300: Loss = 0.046526\n",
            "Step 134300: Loss = 0.047552\n",
            "Step 134400: Loss = 0.049282\n",
            "Step 134400: Loss = 0.040794\n",
            "Step 134500: Loss = 0.050502\n",
            "Step 134500: Loss = 0.049208\n",
            "Step 134600: Loss = 0.039145\n",
            "Step 134600: Loss = 0.045629\n",
            "Step 134700: Loss = 0.048697\n",
            "Step 134700: Loss = 0.048692\n",
            "Step 134800: Loss = 0.051164\n",
            "Step 134800: Loss = 0.050057\n",
            "Step 134900: Loss = 0.046251\n",
            "Step 134900: Loss = 0.047365\n",
            "Step 135000: Loss = 0.047309\n",
            "Step 135000: Loss = 0.046955\n",
            "\n",
            "Saving samples at step 135000\n",
            "Average loss over last 1000 steps: 0.048249\n",
            "Step 135100: Loss = 0.049907\n",
            "Step 135100: Loss = 0.053336\n",
            "Step 135200: Loss = 0.054138\n",
            "Step 135200: Loss = 0.046385\n",
            "Step 135300: Loss = 0.048886\n",
            "Step 135300: Loss = 0.049159\n",
            "Step 135400: Loss = 0.054428\n",
            "Step 135400: Loss = 0.045009\n",
            "Step 135500: Loss = 0.043424\n",
            "Step 135500: Loss = 0.050748\n",
            "Step 135600: Loss = 0.050053\n",
            "Step 135600: Loss = 0.039630\n",
            "Step 135700: Loss = 0.037813\n",
            "Step 135700: Loss = 0.042428\n",
            "Step 135800: Loss = 0.048341\n",
            "Step 135800: Loss = 0.046620\n",
            "Step 135900: Loss = 0.049279\n",
            "Step 135900: Loss = 0.047380\n",
            "Step 136000: Loss = 0.042841\n",
            "Step 136000: Loss = 0.043530\n",
            "\n",
            "Saving samples at step 136000\n",
            "Average loss over last 1000 steps: 0.048091\n",
            "Step 136100: Loss = 0.042894\n",
            "Step 136100: Loss = 0.048932\n",
            "Step 136200: Loss = 0.045454\n",
            "Step 136200: Loss = 0.043241\n",
            "Step 136300: Loss = 0.049472\n",
            "Step 136300: Loss = 0.044577\n",
            "Step 136400: Loss = 0.040548\n",
            "Step 136400: Loss = 0.044532\n",
            "Step 136500: Loss = 0.055261\n",
            "Step 136500: Loss = 0.047587\n",
            "Step 136600: Loss = 0.044029\n",
            "Step 136600: Loss = 0.050212\n",
            "Step 136700: Loss = 0.044262\n",
            "Step 136700: Loss = 0.049670\n",
            "Step 136800: Loss = 0.051033\n",
            "Step 136800: Loss = 0.046287\n",
            "Step 136900: Loss = 0.047959\n",
            "Step 136900: Loss = 0.046222\n",
            "Step 137000: Loss = 0.049242\n",
            "Step 137000: Loss = 0.045310\n",
            "\n",
            "Saving samples at step 137000\n",
            "Average loss over last 1000 steps: 0.047946\n",
            "Step 137100: Loss = 0.045044\n",
            "Step 137100: Loss = 0.046011\n",
            "Step 137200: Loss = 0.039736\n",
            "Step 137200: Loss = 0.051210\n",
            "Step 137300: Loss = 0.042405\n",
            "Step 137300: Loss = 0.047772\n",
            "Step 137400: Loss = 0.051145\n",
            "Step 137400: Loss = 0.051396\n",
            "Step 137500: Loss = 0.047710\n",
            "Step 137500: Loss = 0.052429\n",
            "Step 137600: Loss = 0.048155\n",
            "Step 137600: Loss = 0.046618\n",
            "Step 137700: Loss = 0.044106\n",
            "Step 137700: Loss = 0.046767\n",
            "Step 137800: Loss = 0.052458\n",
            "Step 137800: Loss = 0.043620\n",
            "Step 137900: Loss = 0.049740\n",
            "Step 137900: Loss = 0.046838\n",
            "Step 138000: Loss = 0.041666\n",
            "Step 138000: Loss = 0.050730\n",
            "\n",
            "Saving samples at step 138000\n",
            "Average loss over last 1000 steps: 0.047691\n",
            "Step 138100: Loss = 0.045583\n",
            "Step 138100: Loss = 0.048684\n",
            "Step 138200: Loss = 0.051085\n",
            "Step 138200: Loss = 0.046211\n",
            "Step 138300: Loss = 0.044969\n",
            "Step 138300: Loss = 0.043697\n",
            "Step 138400: Loss = 0.046110\n",
            "Step 138400: Loss = 0.051282\n",
            "Step 138500: Loss = 0.045606\n",
            "Step 138500: Loss = 0.047441\n",
            "Step 138600: Loss = 0.048727\n",
            "Step 138600: Loss = 0.047527\n",
            "Step 138700: Loss = 0.043081\n",
            "Step 138700: Loss = 0.043507\n",
            "Step 138800: Loss = 0.046658\n",
            "Step 138800: Loss = 0.040515\n",
            "Step 138900: Loss = 0.045345\n",
            "Step 138900: Loss = 0.047876\n",
            "Step 139000: Loss = 0.045826\n",
            "Step 139000: Loss = 0.052237\n",
            "\n",
            "Saving samples at step 139000\n",
            "Average loss over last 1000 steps: 0.047529\n",
            "Step 139100: Loss = 0.048720\n",
            "Step 139100: Loss = 0.049256\n",
            "Step 139200: Loss = 0.044809\n",
            "Step 139200: Loss = 0.043194\n",
            "Step 139300: Loss = 0.044603\n",
            "Step 139300: Loss = 0.050598\n",
            "Step 139400: Loss = 0.047258\n",
            "Step 139400: Loss = 0.053647\n",
            "Step 139500: Loss = 0.043978\n",
            "Step 139500: Loss = 0.044662\n",
            "Step 139600: Loss = 0.046429\n",
            "Step 139600: Loss = 0.043816\n",
            "Step 139700: Loss = 0.046040\n",
            "Step 139700: Loss = 0.046869\n",
            "Step 139800: Loss = 0.045881\n",
            "Step 139800: Loss = 0.050651\n",
            "Step 139900: Loss = 0.049725\n",
            "Step 139900: Loss = 0.047120\n",
            "Step 140000: Loss = 0.051105\n",
            "Step 140000: Loss = 0.050376\n",
            "\n",
            "Saving samples at step 140000\n",
            "Average loss over last 1000 steps: 0.047234\n",
            "Model saved at step 140000\n",
            "\n",
            "Evaluating model at step 140000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.02it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.67it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.10it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 140000:\n",
            "Direct reconstruction: SSIM=0.8894, RMSE=0.0824, FID=167.16\n",
            "Deblurred (Algorithm 2): SSIM=0.8223, RMSE=0.1175, FID=161.72\n",
            "Step 140100: Loss = 0.046706\n",
            "Step 140100: Loss = 0.047970\n",
            "Step 140200: Loss = 0.045365\n",
            "Step 140200: Loss = 0.047231\n",
            "Step 140300: Loss = 0.051599\n",
            "Step 140300: Loss = 0.048840\n",
            "Step 140400: Loss = 0.048932\n",
            "Step 140400: Loss = 0.044949\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 140500: Loss = 0.045501\n",
            "Step 140500: Loss = 0.059164\n",
            "Step 140600: Loss = 0.044780\n",
            "Step 140600: Loss = 0.052061\n",
            "Step 140700: Loss = 0.053378\n",
            "Step 140700: Loss = 0.045849\n",
            "Step 140800: Loss = 0.046075\n",
            "Step 140800: Loss = 0.049140\n",
            "Step 140900: Loss = 0.044312\n",
            "Step 140900: Loss = 0.042193\n",
            "Step 141000: Loss = 0.041250\n",
            "Step 141000: Loss = 0.049316\n",
            "\n",
            "Saving samples at step 141000\n",
            "Average loss over last 1000 steps: 0.047162\n",
            "Step 141100: Loss = 0.054058\n",
            "Step 141100: Loss = 0.039757\n",
            "Step 141200: Loss = 0.045897\n",
            "Step 141200: Loss = 0.041824\n",
            "Step 141300: Loss = 0.048781\n",
            "Step 141300: Loss = 0.050944\n",
            "Step 141400: Loss = 0.049455\n",
            "Step 141400: Loss = 0.043074\n",
            "Step 141500: Loss = 0.046852\n",
            "Step 141500: Loss = 0.044051\n",
            "Step 141600: Loss = 0.050910\n",
            "Step 141600: Loss = 0.048987\n",
            "Step 141700: Loss = 0.057186\n",
            "Step 141700: Loss = 0.043217\n",
            "Step 141800: Loss = 0.048123\n",
            "Step 141800: Loss = 0.055546\n",
            "Step 141900: Loss = 0.044772\n",
            "Step 141900: Loss = 0.048426\n",
            "Step 142000: Loss = 0.042239\n",
            "Step 142000: Loss = 0.041996\n",
            "\n",
            "Saving samples at step 142000\n",
            "Average loss over last 1000 steps: 0.047151\n",
            "Step 142100: Loss = 0.041787\n",
            "Step 142100: Loss = 0.049114\n",
            "Step 142200: Loss = 0.059323\n",
            "Step 142200: Loss = 0.041397\n",
            "Step 142300: Loss = 0.051169\n",
            "Step 142300: Loss = 0.048314\n",
            "Step 142400: Loss = 0.044697\n",
            "Step 142400: Loss = 0.055725\n",
            "Step 142500: Loss = 0.046747\n",
            "Step 142500: Loss = 0.041823\n",
            "Step 142600: Loss = 0.049636\n",
            "Step 142600: Loss = 0.040797\n",
            "Step 142700: Loss = 0.047654\n",
            "Step 142700: Loss = 0.040154\n",
            "Step 142800: Loss = 0.046880\n",
            "Step 142800: Loss = 0.041919\n",
            "Step 142900: Loss = 0.042486\n",
            "Step 142900: Loss = 0.047865\n",
            "Step 143000: Loss = 0.044298\n",
            "Step 143000: Loss = 0.046397\n",
            "\n",
            "Saving samples at step 143000\n",
            "Average loss over last 1000 steps: 0.046816\n",
            "Step 143100: Loss = 0.050134\n",
            "Step 143100: Loss = 0.044611\n",
            "Step 143200: Loss = 0.047019\n",
            "Step 143200: Loss = 0.040179\n",
            "Step 143300: Loss = 0.041551\n",
            "Step 143300: Loss = 0.044825\n",
            "Step 143400: Loss = 0.043729\n",
            "Step 143400: Loss = 0.044606\n",
            "Step 143500: Loss = 0.047570\n",
            "Step 143500: Loss = 0.041945\n",
            "Step 143600: Loss = 0.052574\n",
            "Step 143600: Loss = 0.044347\n",
            "Step 143700: Loss = 0.042135\n",
            "Step 143700: Loss = 0.048165\n",
            "Step 143800: Loss = 0.049121\n",
            "Step 143800: Loss = 0.050864\n",
            "Step 143900: Loss = 0.048018\n",
            "Step 143900: Loss = 0.053187\n",
            "Step 144000: Loss = 0.050333\n",
            "Step 144000: Loss = 0.047636\n",
            "\n",
            "Saving samples at step 144000\n",
            "Average loss over last 1000 steps: 0.046778\n",
            "Step 144100: Loss = 0.040163\n",
            "Step 144100: Loss = 0.049533\n",
            "Step 144200: Loss = 0.045233\n",
            "Step 144200: Loss = 0.044606\n",
            "Step 144300: Loss = 0.050829\n",
            "Step 144300: Loss = 0.049227\n",
            "Step 144400: Loss = 0.047537\n",
            "Step 144400: Loss = 0.046142\n",
            "Step 144500: Loss = 0.047976\n",
            "Step 144500: Loss = 0.046925\n",
            "Step 144600: Loss = 0.038437\n",
            "Step 144600: Loss = 0.045389\n",
            "Step 144700: Loss = 0.048064\n",
            "Step 144700: Loss = 0.046862\n",
            "Step 144800: Loss = 0.052630\n",
            "Step 144800: Loss = 0.042819\n",
            "Step 144900: Loss = 0.044468\n",
            "Step 144900: Loss = 0.043473\n",
            "Step 145000: Loss = 0.046501\n",
            "Step 145000: Loss = 0.045322\n",
            "\n",
            "Saving samples at step 145000\n",
            "Average loss over last 1000 steps: 0.046505\n",
            "Step 145100: Loss = 0.044322\n",
            "Step 145100: Loss = 0.050094\n",
            "Step 145200: Loss = 0.044261\n",
            "Step 145200: Loss = 0.050522\n",
            "Step 145300: Loss = 0.042188\n",
            "Step 145300: Loss = 0.048822\n",
            "Step 145400: Loss = 0.038289\n",
            "Step 145400: Loss = 0.041947\n",
            "Step 145500: Loss = 0.045794\n",
            "Step 145500: Loss = 0.043474\n",
            "Step 145600: Loss = 0.046522\n",
            "Step 145600: Loss = 0.047920\n",
            "Step 145700: Loss = 0.046513\n",
            "Step 145700: Loss = 0.041324\n",
            "Step 145800: Loss = 0.044952\n",
            "Step 145800: Loss = 0.052296\n",
            "Step 145900: Loss = 0.043037\n",
            "Step 145900: Loss = 0.043383\n",
            "Step 146000: Loss = 0.047331\n",
            "Step 146000: Loss = 0.043167\n",
            "\n",
            "Saving samples at step 146000\n",
            "Average loss over last 1000 steps: 0.046353\n",
            "Step 146100: Loss = 0.049790\n",
            "Step 146100: Loss = 0.050041\n",
            "Step 146200: Loss = 0.044986\n",
            "Step 146200: Loss = 0.039960\n",
            "Step 146300: Loss = 0.047559\n",
            "Step 146300: Loss = 0.038267\n",
            "Step 146400: Loss = 0.051144\n",
            "Step 146400: Loss = 0.046835\n",
            "Step 146500: Loss = 0.053944\n",
            "Step 146500: Loss = 0.050656\n",
            "Step 146600: Loss = 0.048888\n",
            "Step 146600: Loss = 0.050773\n",
            "Step 146700: Loss = 0.050669\n",
            "Step 146700: Loss = 0.043528\n",
            "Step 146800: Loss = 0.040856\n",
            "Step 146800: Loss = 0.047451\n",
            "Step 146900: Loss = 0.042993\n",
            "Step 146900: Loss = 0.052344\n",
            "Step 147000: Loss = 0.047314\n",
            "Step 147000: Loss = 0.040877\n",
            "\n",
            "Saving samples at step 147000\n",
            "Average loss over last 1000 steps: 0.046318\n",
            "Step 147100: Loss = 0.049131\n",
            "Step 147100: Loss = 0.044179\n",
            "Step 147200: Loss = 0.042044\n",
            "Step 147200: Loss = 0.040470\n",
            "Step 147300: Loss = 0.044383\n",
            "Step 147300: Loss = 0.045843\n",
            "Step 147400: Loss = 0.046740\n",
            "Step 147400: Loss = 0.043799\n",
            "Step 147500: Loss = 0.052246\n",
            "Step 147500: Loss = 0.036907\n",
            "Step 147600: Loss = 0.048332\n",
            "Step 147600: Loss = 0.048146\n",
            "Step 147700: Loss = 0.042621\n",
            "Step 147700: Loss = 0.046770\n",
            "Step 147800: Loss = 0.044239\n",
            "Step 147800: Loss = 0.042911\n",
            "Step 147900: Loss = 0.048809\n",
            "Step 147900: Loss = 0.041112\n",
            "Step 148000: Loss = 0.044215\n",
            "Step 148000: Loss = 0.049671\n",
            "\n",
            "Saving samples at step 148000\n",
            "Average loss over last 1000 steps: 0.045994\n",
            "Step 148100: Loss = 0.048220\n",
            "Step 148100: Loss = 0.038433\n",
            "Step 148200: Loss = 0.042394\n",
            "Step 148200: Loss = 0.048449\n",
            "Step 148300: Loss = 0.048031\n",
            "Step 148300: Loss = 0.052216\n",
            "Step 148400: Loss = 0.046606\n",
            "Step 148400: Loss = 0.039911\n",
            "Step 148500: Loss = 0.044342\n",
            "Step 148500: Loss = 0.047304\n",
            "Step 148600: Loss = 0.046592\n",
            "Step 148600: Loss = 0.043958\n",
            "Step 148700: Loss = 0.052878\n",
            "Step 148700: Loss = 0.044641\n",
            "Step 148800: Loss = 0.045330\n",
            "Step 148800: Loss = 0.043233\n",
            "Step 148900: Loss = 0.048755\n",
            "Step 148900: Loss = 0.051253\n",
            "Step 149000: Loss = 0.042391\n",
            "Step 149000: Loss = 0.047324\n",
            "\n",
            "Saving samples at step 149000\n",
            "Average loss over last 1000 steps: 0.045894\n",
            "Step 149100: Loss = 0.043380\n",
            "Step 149100: Loss = 0.049676\n",
            "Step 149200: Loss = 0.046745\n",
            "Step 149200: Loss = 0.047358\n",
            "Step 149300: Loss = 0.044470\n",
            "Step 149300: Loss = 0.045130\n",
            "Step 149400: Loss = 0.042520\n",
            "Step 149400: Loss = 0.049013\n",
            "Step 149500: Loss = 0.042662\n",
            "Step 149500: Loss = 0.047963\n",
            "Step 149600: Loss = 0.037093\n",
            "Step 149600: Loss = 0.052707\n",
            "Step 149700: Loss = 0.045696\n",
            "Step 149700: Loss = 0.039718\n",
            "Step 149800: Loss = 0.047502\n",
            "Step 149800: Loss = 0.048748\n",
            "Step 149900: Loss = 0.044315\n",
            "Step 149900: Loss = 0.046058\n",
            "Step 150000: Loss = 0.045236\n",
            "Step 150000: Loss = 0.045234\n",
            "\n",
            "Saving samples at step 150000\n",
            "Average loss over last 1000 steps: 0.045731\n",
            "Model saved at step 150000\n",
            "\n",
            "Evaluating model at step 150000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  4.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.80it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.37it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 150000:\n",
            "Direct reconstruction: SSIM=0.8963, RMSE=0.0795, FID=160.59\n",
            "Deblurred (Algorithm 2): SSIM=0.8332, RMSE=0.1131, FID=155.13\n",
            "Step 150100: Loss = 0.043713\n",
            "Step 150100: Loss = 0.046247\n",
            "Step 150200: Loss = 0.044508\n",
            "Step 150200: Loss = 0.048422\n",
            "Step 150300: Loss = 0.046958\n",
            "Step 150300: Loss = 0.044339\n",
            "Step 150400: Loss = 0.052140\n",
            "Step 150400: Loss = 0.047867\n",
            "Step 150500: Loss = 0.041190\n",
            "Step 150500: Loss = 0.048157\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 150600: Loss = 0.042222\n",
            "Step 150600: Loss = 0.043409\n",
            "Step 150700: Loss = 0.047165\n",
            "Step 150700: Loss = 0.040808\n",
            "Step 150800: Loss = 0.050357\n",
            "Step 150800: Loss = 0.040770\n",
            "Step 150900: Loss = 0.045988\n",
            "Step 150900: Loss = 0.042896\n",
            "Step 151000: Loss = 0.041792\n",
            "Step 151000: Loss = 0.051783\n",
            "\n",
            "Saving samples at step 151000\n",
            "Average loss over last 1000 steps: 0.045518\n",
            "Step 151100: Loss = 0.037086\n",
            "Step 151100: Loss = 0.045734\n",
            "Step 151200: Loss = 0.044705\n",
            "Step 151200: Loss = 0.043743\n",
            "Step 151300: Loss = 0.052317\n",
            "Step 151300: Loss = 0.042980\n",
            "Step 151400: Loss = 0.044128\n",
            "Step 151400: Loss = 0.050552\n",
            "Step 151500: Loss = 0.044496\n",
            "Step 151500: Loss = 0.045063\n",
            "Step 151600: Loss = 0.048401\n",
            "Step 151600: Loss = 0.043762\n",
            "Step 151700: Loss = 0.048763\n",
            "Step 151700: Loss = 0.050971\n",
            "Step 151800: Loss = 0.041717\n",
            "Step 151800: Loss = 0.041497\n",
            "Step 151900: Loss = 0.046451\n",
            "Step 151900: Loss = 0.046233\n",
            "Step 152000: Loss = 0.048981\n",
            "Step 152000: Loss = 0.039890\n",
            "\n",
            "Saving samples at step 152000\n",
            "Average loss over last 1000 steps: 0.045423\n",
            "Step 152100: Loss = 0.046012\n",
            "Step 152100: Loss = 0.045323\n",
            "Step 152200: Loss = 0.040363\n",
            "Step 152200: Loss = 0.041110\n",
            "Step 152300: Loss = 0.045229\n",
            "Step 152300: Loss = 0.044319\n",
            "Step 152400: Loss = 0.045174\n",
            "Step 152400: Loss = 0.045075\n",
            "Step 152500: Loss = 0.048170\n",
            "Step 152500: Loss = 0.043025\n",
            "Step 152600: Loss = 0.048059\n",
            "Step 152600: Loss = 0.042633\n",
            "Step 152700: Loss = 0.042134\n",
            "Step 152700: Loss = 0.046934\n",
            "Step 152800: Loss = 0.039672\n",
            "Step 152800: Loss = 0.039739\n",
            "Step 152900: Loss = 0.046706\n",
            "Step 152900: Loss = 0.035205\n",
            "Step 153000: Loss = 0.043858\n",
            "Step 153000: Loss = 0.045499\n",
            "\n",
            "Saving samples at step 153000\n",
            "Average loss over last 1000 steps: 0.045120\n",
            "Step 153100: Loss = 0.045428\n",
            "Step 153100: Loss = 0.042552\n",
            "Step 153200: Loss = 0.046340\n",
            "Step 153200: Loss = 0.045990\n",
            "Step 153300: Loss = 0.036335\n",
            "Step 153300: Loss = 0.042914\n",
            "Step 153400: Loss = 0.047971\n",
            "Step 153400: Loss = 0.046982\n",
            "Step 153500: Loss = 0.044161\n",
            "Step 153500: Loss = 0.043415\n",
            "Step 153600: Loss = 0.052874\n",
            "Step 153600: Loss = 0.045417\n",
            "Step 153700: Loss = 0.039873\n",
            "Step 153700: Loss = 0.051377\n",
            "Step 153800: Loss = 0.042223\n",
            "Step 153800: Loss = 0.045103\n",
            "Step 153900: Loss = 0.047621\n",
            "Step 153900: Loss = 0.047001\n",
            "Step 154000: Loss = 0.044031\n",
            "Step 154000: Loss = 0.047146\n",
            "\n",
            "Saving samples at step 154000\n",
            "Average loss over last 1000 steps: 0.045231\n",
            "Step 154100: Loss = 0.042482\n",
            "Step 154100: Loss = 0.042360\n",
            "Step 154200: Loss = 0.047696\n",
            "Step 154200: Loss = 0.041368\n",
            "Step 154300: Loss = 0.045432\n",
            "Step 154300: Loss = 0.047986\n",
            "Step 154400: Loss = 0.043947\n",
            "Step 154400: Loss = 0.045700\n",
            "Step 154500: Loss = 0.041523\n",
            "Step 154500: Loss = 0.046029\n",
            "Step 154600: Loss = 0.048609\n",
            "Step 154600: Loss = 0.045962\n",
            "Step 154700: Loss = 0.041498\n",
            "Step 154700: Loss = 0.042030\n",
            "Step 154800: Loss = 0.051472\n",
            "Step 154800: Loss = 0.043097\n",
            "Step 154900: Loss = 0.049692\n",
            "Step 154900: Loss = 0.047700\n",
            "Step 155000: Loss = 0.049408\n",
            "Step 155000: Loss = 0.038419\n",
            "\n",
            "Saving samples at step 155000\n",
            "Average loss over last 1000 steps: 0.044935\n",
            "Step 155100: Loss = 0.045198\n",
            "Step 155100: Loss = 0.046748\n",
            "Step 155200: Loss = 0.044257\n",
            "Step 155200: Loss = 0.040379\n",
            "Step 155300: Loss = 0.049953\n",
            "Step 155300: Loss = 0.041925\n",
            "Step 155400: Loss = 0.038247\n",
            "Step 155400: Loss = 0.047576\n",
            "Step 155500: Loss = 0.036841\n",
            "Step 155500: Loss = 0.048518\n",
            "Step 155600: Loss = 0.049943\n",
            "Step 155600: Loss = 0.044758\n",
            "Step 155700: Loss = 0.050071\n",
            "Step 155700: Loss = 0.048460\n",
            "Step 155800: Loss = 0.041557\n",
            "Step 155800: Loss = 0.040715\n",
            "Step 155900: Loss = 0.038328\n",
            "Step 155900: Loss = 0.044995\n",
            "Step 156000: Loss = 0.038505\n",
            "Step 156000: Loss = 0.045242\n",
            "\n",
            "Saving samples at step 156000\n",
            "Average loss over last 1000 steps: 0.044855\n",
            "Step 156100: Loss = 0.041812\n",
            "Step 156100: Loss = 0.045665\n",
            "Step 156200: Loss = 0.046759\n",
            "Step 156200: Loss = 0.039390\n",
            "Step 156300: Loss = 0.044159\n",
            "Step 156300: Loss = 0.047028\n",
            "Step 156400: Loss = 0.044120\n",
            "Step 156400: Loss = 0.038844\n",
            "Step 156500: Loss = 0.042364\n",
            "Step 156500: Loss = 0.044218\n",
            "Step 156600: Loss = 0.042818\n",
            "Step 156600: Loss = 0.045674\n",
            "Step 156700: Loss = 0.042358\n",
            "Step 156700: Loss = 0.046390\n",
            "Step 156800: Loss = 0.041653\n",
            "Step 156800: Loss = 0.045494\n",
            "Step 156900: Loss = 0.040884\n",
            "Step 156900: Loss = 0.045322\n",
            "Step 157000: Loss = 0.041544\n",
            "Step 157000: Loss = 0.049336\n",
            "\n",
            "Saving samples at step 157000\n",
            "Average loss over last 1000 steps: 0.044696\n",
            "Step 157100: Loss = 0.048038\n",
            "Step 157100: Loss = 0.041738\n",
            "Step 157200: Loss = 0.045446\n",
            "Step 157200: Loss = 0.043775\n",
            "Step 157300: Loss = 0.044525\n",
            "Step 157300: Loss = 0.039886\n",
            "Step 157400: Loss = 0.048789\n",
            "Step 157400: Loss = 0.043659\n",
            "Step 157500: Loss = 0.046651\n",
            "Step 157500: Loss = 0.048402\n",
            "Step 157600: Loss = 0.042677\n",
            "Step 157600: Loss = 0.038069\n",
            "Step 157700: Loss = 0.044760\n",
            "Step 157700: Loss = 0.044884\n",
            "Step 157800: Loss = 0.047595\n",
            "Step 157800: Loss = 0.040546\n",
            "Step 157900: Loss = 0.045966\n",
            "Step 157900: Loss = 0.046940\n",
            "Step 158000: Loss = 0.045581\n",
            "Step 158000: Loss = 0.044786\n",
            "\n",
            "Saving samples at step 158000\n",
            "Average loss over last 1000 steps: 0.044569\n",
            "Step 158100: Loss = 0.039447\n",
            "Step 158100: Loss = 0.043081\n",
            "Step 158200: Loss = 0.038576\n",
            "Step 158200: Loss = 0.043913\n",
            "Step 158300: Loss = 0.046059\n",
            "Step 158300: Loss = 0.040728\n",
            "Step 158400: Loss = 0.042169\n",
            "Step 158400: Loss = 0.046977\n",
            "Step 158500: Loss = 0.044968\n",
            "Step 158500: Loss = 0.044036\n",
            "Step 158600: Loss = 0.041797\n",
            "Step 158600: Loss = 0.045696\n",
            "Step 158700: Loss = 0.045386\n",
            "Step 158700: Loss = 0.043883\n",
            "Step 158800: Loss = 0.041702\n",
            "Step 158800: Loss = 0.039627\n",
            "Step 158900: Loss = 0.038932\n",
            "Step 158900: Loss = 0.047518\n",
            "Step 159000: Loss = 0.048607\n",
            "Step 159000: Loss = 0.045651\n",
            "\n",
            "Saving samples at step 159000\n",
            "Average loss over last 1000 steps: 0.044419\n",
            "Step 159100: Loss = 0.041682\n",
            "Step 159100: Loss = 0.046202\n",
            "Step 159200: Loss = 0.041315\n",
            "Step 159200: Loss = 0.042077\n",
            "Step 159300: Loss = 0.041782\n",
            "Step 159300: Loss = 0.045915\n",
            "Step 159400: Loss = 0.047486\n",
            "Step 159400: Loss = 0.049595\n",
            "Step 159500: Loss = 0.039496\n",
            "Step 159500: Loss = 0.044667\n",
            "Step 159600: Loss = 0.045854\n",
            "Step 159600: Loss = 0.043338\n",
            "Step 159700: Loss = 0.045419\n",
            "Step 159700: Loss = 0.042674\n",
            "Step 159800: Loss = 0.047448\n",
            "Step 159800: Loss = 0.043080\n",
            "Step 159900: Loss = 0.038918\n",
            "Step 159900: Loss = 0.049435\n",
            "Step 160000: Loss = 0.045965\n",
            "Step 160000: Loss = 0.041959\n",
            "\n",
            "Saving samples at step 160000\n",
            "Average loss over last 1000 steps: 0.044228\n",
            "Model saved at step 160000\n",
            "\n",
            "Evaluating model at step 160000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  4.03it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.04it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.81it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.05it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 160000:\n",
            "Direct reconstruction: SSIM=0.9031, RMSE=0.0763, FID=154.54\n",
            "Deblurred (Algorithm 2): SSIM=0.8417, RMSE=0.1098, FID=151.16\n",
            "Step 160100: Loss = 0.047385\n",
            "Step 160100: Loss = 0.043758\n",
            "Step 160200: Loss = 0.048927\n",
            "Step 160200: Loss = 0.049143\n",
            "Step 160300: Loss = 0.037205\n",
            "Step 160300: Loss = 0.043998\n",
            "Step 160400: Loss = 0.040812\n",
            "Step 160400: Loss = 0.044180\n",
            "Step 160500: Loss = 0.048836\n",
            "Step 160500: Loss = 0.038799\n",
            "Step 160600: Loss = 0.045133\n",
            "Step 160600: Loss = 0.046070\n",
            "Step 160700: Loss = 0.043212\n",
            "Step 160700: Loss = 0.042303\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 160800: Loss = 0.045549\n",
            "Step 160800: Loss = 0.043007\n",
            "Step 160900: Loss = 0.042268\n",
            "Step 160900: Loss = 0.045394\n",
            "Step 161000: Loss = 0.042596\n",
            "Step 161000: Loss = 0.047540\n",
            "\n",
            "Saving samples at step 161000\n",
            "Average loss over last 1000 steps: 0.044124\n",
            "Step 161100: Loss = 0.044040\n",
            "Step 161100: Loss = 0.041808\n",
            "Step 161200: Loss = 0.044820\n",
            "Step 161200: Loss = 0.042469\n",
            "Step 161300: Loss = 0.044699\n",
            "Step 161300: Loss = 0.045813\n",
            "Step 161400: Loss = 0.046349\n",
            "Step 161400: Loss = 0.043504\n",
            "Step 161500: Loss = 0.047051\n",
            "Step 161500: Loss = 0.048008\n",
            "Step 161600: Loss = 0.040829\n",
            "Step 161600: Loss = 0.045857\n",
            "Step 161700: Loss = 0.044005\n",
            "Step 161700: Loss = 0.045147\n",
            "Step 161800: Loss = 0.044409\n",
            "Step 161800: Loss = 0.051474\n",
            "Step 161900: Loss = 0.044887\n",
            "Step 161900: Loss = 0.049844\n",
            "Step 162000: Loss = 0.047906\n",
            "Step 162000: Loss = 0.044737\n",
            "\n",
            "Saving samples at step 162000\n",
            "Average loss over last 1000 steps: 0.043939\n",
            "Step 162100: Loss = 0.051308\n",
            "Step 162100: Loss = 0.048866\n",
            "Step 162200: Loss = 0.042198\n",
            "Step 162200: Loss = 0.038125\n",
            "Step 162300: Loss = 0.043934\n",
            "Step 162300: Loss = 0.044235\n",
            "Step 162400: Loss = 0.050363\n",
            "Step 162400: Loss = 0.047834\n",
            "Step 162500: Loss = 0.042863\n",
            "Step 162500: Loss = 0.041878\n",
            "Step 162600: Loss = 0.051280\n",
            "Step 162600: Loss = 0.042015\n",
            "Step 162700: Loss = 0.043058\n",
            "Step 162700: Loss = 0.042731\n",
            "Step 162800: Loss = 0.041840\n",
            "Step 162800: Loss = 0.045365\n",
            "Step 162900: Loss = 0.046229\n",
            "Step 162900: Loss = 0.040544\n",
            "Step 163000: Loss = 0.043544\n",
            "Step 163000: Loss = 0.040741\n",
            "\n",
            "Saving samples at step 163000\n",
            "Average loss over last 1000 steps: 0.043922\n",
            "Step 163100: Loss = 0.040185\n",
            "Step 163100: Loss = 0.044556\n",
            "Step 163200: Loss = 0.041489\n",
            "Step 163200: Loss = 0.045265\n",
            "Step 163300: Loss = 0.038749\n",
            "Step 163300: Loss = 0.046253\n",
            "Step 163400: Loss = 0.047647\n",
            "Step 163400: Loss = 0.041559\n",
            "Step 163500: Loss = 0.044429\n",
            "Step 163500: Loss = 0.044220\n",
            "Step 163600: Loss = 0.047297\n",
            "Step 163600: Loss = 0.051968\n",
            "Step 163700: Loss = 0.045343\n",
            "Step 163700: Loss = 0.039149\n",
            "Step 163800: Loss = 0.046135\n",
            "Step 163800: Loss = 0.045358\n",
            "Step 163900: Loss = 0.043695\n",
            "Step 163900: Loss = 0.046931\n",
            "Step 164000: Loss = 0.048073\n",
            "Step 164000: Loss = 0.045944\n",
            "\n",
            "Saving samples at step 164000\n",
            "Average loss over last 1000 steps: 0.043730\n",
            "Step 164100: Loss = 0.043209\n",
            "Step 164100: Loss = 0.046720\n",
            "Step 164200: Loss = 0.044229\n",
            "Step 164200: Loss = 0.039222\n",
            "Step 164300: Loss = 0.037883\n",
            "Step 164300: Loss = 0.044997\n",
            "Step 164400: Loss = 0.038194\n",
            "Step 164400: Loss = 0.040244\n",
            "Step 164500: Loss = 0.041370\n",
            "Step 164500: Loss = 0.048323\n",
            "Step 164600: Loss = 0.047621\n",
            "Step 164600: Loss = 0.051682\n",
            "Step 164700: Loss = 0.041743\n",
            "Step 164700: Loss = 0.039823\n",
            "Step 164800: Loss = 0.038481\n",
            "Step 164800: Loss = 0.044688\n",
            "Step 164900: Loss = 0.048704\n",
            "Step 164900: Loss = 0.050182\n",
            "Step 165000: Loss = 0.039728\n",
            "Step 165000: Loss = 0.038136\n",
            "\n",
            "Saving samples at step 165000\n",
            "Average loss over last 1000 steps: 0.043642\n",
            "Step 165100: Loss = 0.043145\n",
            "Step 165100: Loss = 0.041468\n",
            "Step 165200: Loss = 0.040542\n",
            "Step 165200: Loss = 0.043316\n",
            "Step 165300: Loss = 0.042784\n",
            "Step 165300: Loss = 0.043461\n",
            "Step 165400: Loss = 0.040935\n",
            "Step 165400: Loss = 0.040669\n",
            "Step 165500: Loss = 0.042725\n",
            "Step 165500: Loss = 0.041437\n",
            "Step 165600: Loss = 0.038417\n",
            "Step 165600: Loss = 0.044471\n",
            "Step 165700: Loss = 0.043243\n",
            "Step 165700: Loss = 0.045947\n",
            "Step 165800: Loss = 0.037901\n",
            "Step 165800: Loss = 0.044431\n",
            "Step 165900: Loss = 0.040568\n",
            "Step 165900: Loss = 0.041404\n",
            "Step 166000: Loss = 0.048539\n",
            "Step 166000: Loss = 0.046302\n",
            "\n",
            "Saving samples at step 166000\n",
            "Average loss over last 1000 steps: 0.043446\n",
            "Step 166100: Loss = 0.046032\n",
            "Step 166100: Loss = 0.039138\n",
            "Step 166200: Loss = 0.037294\n",
            "Step 166200: Loss = 0.039102\n",
            "Step 166300: Loss = 0.046265\n",
            "Step 166300: Loss = 0.040725\n",
            "Step 166400: Loss = 0.044222\n",
            "Step 166400: Loss = 0.044597\n",
            "Step 166500: Loss = 0.043895\n",
            "Step 166500: Loss = 0.038364\n",
            "Step 166600: Loss = 0.042856\n",
            "Step 166600: Loss = 0.048175\n",
            "Step 166700: Loss = 0.042943\n",
            "Step 166700: Loss = 0.041018\n",
            "Step 166800: Loss = 0.042490\n",
            "Step 166800: Loss = 0.044993\n",
            "Step 166900: Loss = 0.041731\n",
            "Step 166900: Loss = 0.034721\n",
            "Step 167000: Loss = 0.037891\n",
            "Step 167000: Loss = 0.041284\n",
            "\n",
            "Saving samples at step 167000\n",
            "Average loss over last 1000 steps: 0.043296\n",
            "Step 167100: Loss = 0.046815\n",
            "Step 167100: Loss = 0.045368\n",
            "Step 167200: Loss = 0.043292\n",
            "Step 167200: Loss = 0.044012\n",
            "Step 167300: Loss = 0.041720\n",
            "Step 167300: Loss = 0.048491\n",
            "Step 167400: Loss = 0.045687\n",
            "Step 167400: Loss = 0.042015\n",
            "Step 167500: Loss = 0.045507\n",
            "Step 167500: Loss = 0.040663\n",
            "Step 167600: Loss = 0.039212\n",
            "Step 167600: Loss = 0.046182\n",
            "Step 167700: Loss = 0.036456\n",
            "Step 167700: Loss = 0.040438\n",
            "Step 167800: Loss = 0.051870\n",
            "Step 167800: Loss = 0.052461\n",
            "Step 167900: Loss = 0.038285\n",
            "Step 167900: Loss = 0.042088\n",
            "Step 168000: Loss = 0.041760\n",
            "Step 168000: Loss = 0.042364\n",
            "\n",
            "Saving samples at step 168000\n",
            "Average loss over last 1000 steps: 0.043212\n",
            "Step 168100: Loss = 0.044717\n",
            "Step 168100: Loss = 0.043441\n",
            "Step 168200: Loss = 0.046842\n",
            "Step 168200: Loss = 0.042105\n",
            "Step 168300: Loss = 0.045246\n",
            "Step 168300: Loss = 0.041809\n",
            "Step 168400: Loss = 0.043529\n",
            "Step 168400: Loss = 0.039639\n",
            "Step 168500: Loss = 0.045870\n",
            "Step 168500: Loss = 0.043989\n",
            "Step 168600: Loss = 0.042246\n",
            "Step 168600: Loss = 0.044636\n",
            "Step 168700: Loss = 0.038350\n",
            "Step 168700: Loss = 0.041368\n",
            "Step 168800: Loss = 0.045199\n",
            "Step 168800: Loss = 0.042617\n",
            "Step 168900: Loss = 0.043943\n",
            "Step 168900: Loss = 0.045610\n",
            "Step 169000: Loss = 0.040939\n",
            "Step 169000: Loss = 0.042112\n",
            "\n",
            "Saving samples at step 169000\n",
            "Average loss over last 1000 steps: 0.042944\n",
            "Step 169100: Loss = 0.044112\n",
            "Step 169100: Loss = 0.042000\n",
            "Step 169200: Loss = 0.045359\n",
            "Step 169200: Loss = 0.043363\n",
            "Step 169300: Loss = 0.043161\n",
            "Step 169300: Loss = 0.046042\n",
            "Step 169400: Loss = 0.036424\n",
            "Step 169400: Loss = 0.043078\n",
            "Step 169500: Loss = 0.035389\n",
            "Step 169500: Loss = 0.045687\n",
            "Step 169600: Loss = 0.045315\n",
            "Step 169600: Loss = 0.044653\n",
            "Step 169700: Loss = 0.041643\n",
            "Step 169700: Loss = 0.044682\n",
            "Step 169800: Loss = 0.042268\n",
            "Step 169800: Loss = 0.042101\n",
            "Step 169900: Loss = 0.040080\n",
            "Step 169900: Loss = 0.048752\n",
            "Step 170000: Loss = 0.043018\n",
            "Step 170000: Loss = 0.039097\n",
            "\n",
            "Saving samples at step 170000\n",
            "Average loss over last 1000 steps: 0.043020\n",
            "Model saved at step 170000\n",
            "\n",
            "Evaluating model at step 170000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.94it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.91it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.72it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.98it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.39it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 170000:\n",
            "Direct reconstruction: SSIM=0.9081, RMSE=0.0741, FID=151.13\n",
            "Deblurred (Algorithm 2): SSIM=0.8500, RMSE=0.1065, FID=149.17\n",
            "Step 170100: Loss = 0.042277\n",
            "Step 170100: Loss = 0.042799\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 170200: Loss = 0.037417\n",
            "Step 170200: Loss = 0.047957\n",
            "Step 170300: Loss = 0.043363\n",
            "Step 170300: Loss = 0.045428\n",
            "Step 170400: Loss = 0.039620\n",
            "Step 170400: Loss = 0.050143\n",
            "Step 170500: Loss = 0.044939\n",
            "Step 170500: Loss = 0.041869\n",
            "Step 170600: Loss = 0.049081\n",
            "Step 170600: Loss = 0.046443\n",
            "Step 170700: Loss = 0.040678\n",
            "Step 170700: Loss = 0.047152\n",
            "Step 170800: Loss = 0.044784\n",
            "Step 170800: Loss = 0.041134\n",
            "Step 170900: Loss = 0.046071\n",
            "Step 170900: Loss = 0.036457\n",
            "Step 171000: Loss = 0.041792\n",
            "Step 171000: Loss = 0.046097\n",
            "\n",
            "Saving samples at step 171000\n",
            "Average loss over last 1000 steps: 0.042803\n",
            "Step 171100: Loss = 0.039196\n",
            "Step 171100: Loss = 0.041594\n",
            "Step 171200: Loss = 0.036640\n",
            "Step 171200: Loss = 0.043550\n",
            "Step 171300: Loss = 0.045002\n",
            "Step 171300: Loss = 0.046696\n",
            "Step 171400: Loss = 0.039002\n",
            "Step 171400: Loss = 0.043285\n",
            "Step 171500: Loss = 0.042301\n",
            "Step 171500: Loss = 0.041602\n",
            "Step 171600: Loss = 0.041421\n",
            "Step 171600: Loss = 0.043692\n",
            "Step 171700: Loss = 0.047912\n",
            "Step 171700: Loss = 0.039891\n",
            "Step 171800: Loss = 0.036647\n",
            "Step 171800: Loss = 0.041908\n",
            "Step 171900: Loss = 0.045087\n",
            "Step 171900: Loss = 0.040891\n",
            "Step 172000: Loss = 0.042112\n",
            "Step 172000: Loss = 0.038802\n",
            "\n",
            "Saving samples at step 172000\n",
            "Average loss over last 1000 steps: 0.042798\n",
            "Step 172100: Loss = 0.044779\n",
            "Step 172100: Loss = 0.042178\n",
            "Step 172200: Loss = 0.042141\n",
            "Step 172200: Loss = 0.038691\n",
            "Step 172300: Loss = 0.043709\n",
            "Step 172300: Loss = 0.043764\n",
            "Step 172400: Loss = 0.041737\n",
            "Step 172400: Loss = 0.047892\n",
            "Step 172500: Loss = 0.038424\n",
            "Step 172500: Loss = 0.044884\n",
            "Step 172600: Loss = 0.049466\n",
            "Step 172600: Loss = 0.041283\n",
            "Step 172700: Loss = 0.052897\n",
            "Step 172700: Loss = 0.042461\n",
            "Step 172800: Loss = 0.035574\n",
            "Step 172800: Loss = 0.041940\n",
            "Step 172900: Loss = 0.039531\n",
            "Step 172900: Loss = 0.041243\n",
            "Step 173000: Loss = 0.043569\n",
            "Step 173000: Loss = 0.042722\n",
            "\n",
            "Saving samples at step 173000\n",
            "Average loss over last 1000 steps: 0.042498\n",
            "Step 173100: Loss = 0.042805\n",
            "Step 173100: Loss = 0.045458\n",
            "Step 173200: Loss = 0.044821\n",
            "Step 173200: Loss = 0.041972\n",
            "Step 173300: Loss = 0.041145\n",
            "Step 173300: Loss = 0.040385\n",
            "Step 173400: Loss = 0.043517\n",
            "Step 173400: Loss = 0.046356\n",
            "Step 173500: Loss = 0.050879\n",
            "Step 173500: Loss = 0.041659\n",
            "Step 173600: Loss = 0.044252\n",
            "Step 173600: Loss = 0.048740\n",
            "Step 173700: Loss = 0.043850\n",
            "Step 173700: Loss = 0.037059\n",
            "Step 173800: Loss = 0.039806\n",
            "Step 173800: Loss = 0.040991\n",
            "Step 173900: Loss = 0.037641\n",
            "Step 173900: Loss = 0.041294\n",
            "Step 174000: Loss = 0.039119\n",
            "Step 174000: Loss = 0.042420\n",
            "\n",
            "Saving samples at step 174000\n",
            "Average loss over last 1000 steps: 0.042574\n",
            "Step 174100: Loss = 0.046067\n",
            "Step 174100: Loss = 0.041673\n",
            "Step 174200: Loss = 0.035823\n",
            "Step 174200: Loss = 0.040634\n",
            "Step 174300: Loss = 0.046039\n",
            "Step 174300: Loss = 0.040027\n",
            "Step 174400: Loss = 0.046306\n",
            "Step 174400: Loss = 0.044643\n",
            "Step 174500: Loss = 0.044276\n",
            "Step 174500: Loss = 0.047611\n",
            "Step 174600: Loss = 0.035270\n",
            "Step 174600: Loss = 0.041773\n",
            "Step 174700: Loss = 0.041016\n",
            "Step 174700: Loss = 0.047903\n",
            "Step 174800: Loss = 0.040023\n",
            "Step 174800: Loss = 0.041613\n",
            "Step 174900: Loss = 0.044341\n",
            "Step 174900: Loss = 0.033725\n",
            "Step 175000: Loss = 0.048044\n",
            "Step 175000: Loss = 0.041714\n",
            "\n",
            "Saving samples at step 175000\n",
            "Average loss over last 1000 steps: 0.042322\n",
            "Step 175100: Loss = 0.036071\n",
            "Step 175100: Loss = 0.047808\n",
            "Step 175200: Loss = 0.048660\n",
            "Step 175200: Loss = 0.044041\n",
            "Step 175300: Loss = 0.045607\n",
            "Step 175300: Loss = 0.039846\n",
            "Step 175400: Loss = 0.045036\n",
            "Step 175400: Loss = 0.041576\n",
            "Step 175500: Loss = 0.043932\n",
            "Step 175500: Loss = 0.044400\n",
            "Step 175600: Loss = 0.045454\n",
            "Step 175600: Loss = 0.042623\n",
            "Step 175700: Loss = 0.042367\n",
            "Step 175700: Loss = 0.040466\n",
            "Step 175800: Loss = 0.039876\n",
            "Step 175800: Loss = 0.038264\n",
            "Step 175900: Loss = 0.045189\n",
            "Step 175900: Loss = 0.039814\n",
            "Step 176000: Loss = 0.045818\n",
            "Step 176000: Loss = 0.047987\n",
            "\n",
            "Saving samples at step 176000\n",
            "Average loss over last 1000 steps: 0.042337\n",
            "Step 176100: Loss = 0.038309\n",
            "Step 176100: Loss = 0.044763\n",
            "Step 176200: Loss = 0.040816\n",
            "Step 176200: Loss = 0.045497\n",
            "Step 176300: Loss = 0.039362\n",
            "Step 176300: Loss = 0.041646\n",
            "Step 176400: Loss = 0.043789\n",
            "Step 176400: Loss = 0.036303\n",
            "Step 176500: Loss = 0.042590\n",
            "Step 176500: Loss = 0.041649\n",
            "Step 176600: Loss = 0.041371\n",
            "Step 176600: Loss = 0.042655\n",
            "Step 176700: Loss = 0.036086\n",
            "Step 176700: Loss = 0.040698\n",
            "Step 176800: Loss = 0.038972\n",
            "Step 176800: Loss = 0.039087\n",
            "Step 176900: Loss = 0.040219\n",
            "Step 176900: Loss = 0.043800\n",
            "Step 177000: Loss = 0.043397\n",
            "Step 177000: Loss = 0.043978\n",
            "\n",
            "Saving samples at step 177000\n",
            "Average loss over last 1000 steps: 0.042014\n",
            "Step 177100: Loss = 0.035795\n",
            "Step 177100: Loss = 0.042861\n",
            "Step 177200: Loss = 0.037826\n",
            "Step 177200: Loss = 0.045317\n",
            "Step 177300: Loss = 0.037627\n",
            "Step 177300: Loss = 0.038131\n",
            "Step 177400: Loss = 0.041372\n",
            "Step 177400: Loss = 0.036338\n",
            "Step 177500: Loss = 0.044206\n",
            "Step 177500: Loss = 0.043103\n",
            "Step 177600: Loss = 0.039910\n",
            "Step 177600: Loss = 0.039800\n",
            "Step 177700: Loss = 0.047473\n",
            "Step 177700: Loss = 0.044042\n",
            "Step 177800: Loss = 0.044346\n",
            "Step 177800: Loss = 0.040101\n",
            "Step 177900: Loss = 0.045352\n",
            "Step 177900: Loss = 0.043107\n",
            "Step 178000: Loss = 0.043737\n",
            "Step 178000: Loss = 0.042739\n",
            "\n",
            "Saving samples at step 178000\n",
            "Average loss over last 1000 steps: 0.041964\n",
            "Step 178100: Loss = 0.045424\n",
            "Step 178100: Loss = 0.045811\n",
            "Step 178200: Loss = 0.045252\n",
            "Step 178200: Loss = 0.037470\n",
            "Step 178300: Loss = 0.047679\n",
            "Step 178300: Loss = 0.044816\n",
            "Step 178400: Loss = 0.046782\n",
            "Step 178400: Loss = 0.044158\n",
            "Step 178500: Loss = 0.038149\n",
            "Step 178500: Loss = 0.042748\n",
            "Step 178600: Loss = 0.044515\n",
            "Step 178600: Loss = 0.034315\n",
            "Step 178700: Loss = 0.039469\n",
            "Step 178700: Loss = 0.041803\n",
            "Step 178800: Loss = 0.041428\n",
            "Step 178800: Loss = 0.038946\n",
            "Step 178900: Loss = 0.046316\n",
            "Step 178900: Loss = 0.036955\n",
            "Step 179000: Loss = 0.039210\n",
            "Step 179000: Loss = 0.040209\n",
            "\n",
            "Saving samples at step 179000\n",
            "Average loss over last 1000 steps: 0.041852\n",
            "Step 179100: Loss = 0.037839\n",
            "Step 179100: Loss = 0.046410\n",
            "Step 179200: Loss = 0.041107\n",
            "Step 179200: Loss = 0.039490\n",
            "Step 179300: Loss = 0.037351\n",
            "Step 179300: Loss = 0.044113\n",
            "Step 179400: Loss = 0.044765\n",
            "Step 179400: Loss = 0.041781\n",
            "Step 179500: Loss = 0.041961\n",
            "Step 179500: Loss = 0.042046\n",
            "Step 179600: Loss = 0.043130\n",
            "Step 179600: Loss = 0.043151\n",
            "Step 179700: Loss = 0.035045\n",
            "Step 179700: Loss = 0.037366\n",
            "Step 179800: Loss = 0.040070\n",
            "Step 179800: Loss = 0.039223\n",
            "Step 179900: Loss = 0.040684\n",
            "Step 179900: Loss = 0.039664\n",
            "Step 180000: Loss = 0.040112\n",
            "Step 180000: Loss = 0.045138\n",
            "\n",
            "Saving samples at step 180000\n",
            "Average loss over last 1000 steps: 0.041755\n",
            "Model saved at step 180000\n",
            "\n",
            "Evaluating model at step 180000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.59it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.22it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.95it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.29it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  4.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 180000:\n",
            "Direct reconstruction: SSIM=0.9133, RMSE=0.0717, FID=145.47\n",
            "Deblurred (Algorithm 2): SSIM=0.8558, RMSE=0.1042, FID=147.21\n",
            "Step 180100: Loss = 0.041246\n",
            "Step 180100: Loss = 0.044864\n",
            "Step 180200: Loss = 0.041006\n",
            "Step 180200: Loss = 0.040412\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 180300: Loss = 0.043467\n",
            "Step 180300: Loss = 0.037406\n",
            "Step 180400: Loss = 0.050416\n",
            "Step 180400: Loss = 0.045024\n",
            "Step 180500: Loss = 0.039613\n",
            "Step 180500: Loss = 0.035748\n",
            "Step 180600: Loss = 0.039618\n",
            "Step 180600: Loss = 0.042142\n",
            "Step 180700: Loss = 0.049574\n",
            "Step 180700: Loss = 0.042445\n",
            "Step 180800: Loss = 0.041276\n",
            "Step 180800: Loss = 0.042486\n",
            "Step 180900: Loss = 0.039332\n",
            "Step 180900: Loss = 0.042858\n",
            "Step 181000: Loss = 0.035779\n",
            "Step 181000: Loss = 0.041534\n",
            "\n",
            "Saving samples at step 181000\n",
            "Average loss over last 1000 steps: 0.041582\n",
            "Step 181100: Loss = 0.044255\n",
            "Step 181100: Loss = 0.047648\n",
            "Step 181200: Loss = 0.037442\n",
            "Step 181200: Loss = 0.038199\n",
            "Step 181300: Loss = 0.042162\n",
            "Step 181300: Loss = 0.047838\n",
            "Step 181400: Loss = 0.049963\n",
            "Step 181400: Loss = 0.039337\n",
            "Step 181500: Loss = 0.043777\n",
            "Step 181500: Loss = 0.039052\n",
            "Step 181600: Loss = 0.045141\n",
            "Step 181600: Loss = 0.045497\n",
            "Step 181700: Loss = 0.043059\n",
            "Step 181700: Loss = 0.045304\n",
            "Step 181800: Loss = 0.047608\n",
            "Step 181800: Loss = 0.042852\n",
            "Step 181900: Loss = 0.040093\n",
            "Step 181900: Loss = 0.043781\n",
            "Step 182000: Loss = 0.042454\n",
            "Step 182000: Loss = 0.037740\n",
            "\n",
            "Saving samples at step 182000\n",
            "Average loss over last 1000 steps: 0.041603\n",
            "Step 182100: Loss = 0.046521\n",
            "Step 182100: Loss = 0.041760\n",
            "Step 182200: Loss = 0.045047\n",
            "Step 182200: Loss = 0.041144\n",
            "Step 182300: Loss = 0.050288\n",
            "Step 182300: Loss = 0.042263\n",
            "Step 182400: Loss = 0.039067\n",
            "Step 182400: Loss = 0.045915\n",
            "Step 182500: Loss = 0.046767\n",
            "Step 182500: Loss = 0.040426\n",
            "Step 182600: Loss = 0.037750\n",
            "Step 182600: Loss = 0.045074\n",
            "Step 182700: Loss = 0.037276\n",
            "Step 182700: Loss = 0.040169\n",
            "Step 182800: Loss = 0.042314\n",
            "Step 182800: Loss = 0.044924\n",
            "Step 182900: Loss = 0.039002\n",
            "Step 182900: Loss = 0.042252\n",
            "Step 183000: Loss = 0.046485\n",
            "Step 183000: Loss = 0.037387\n",
            "\n",
            "Saving samples at step 183000\n",
            "Average loss over last 1000 steps: 0.041435\n",
            "Step 183100: Loss = 0.045973\n",
            "Step 183100: Loss = 0.041343\n",
            "Step 183200: Loss = 0.042418\n",
            "Step 183200: Loss = 0.040491\n",
            "Step 183300: Loss = 0.039129\n",
            "Step 183300: Loss = 0.043015\n",
            "Step 183400: Loss = 0.041024\n",
            "Step 183400: Loss = 0.043349\n",
            "Step 183500: Loss = 0.043622\n",
            "Step 183500: Loss = 0.040178\n",
            "Step 183600: Loss = 0.037123\n",
            "Step 183600: Loss = 0.039295\n",
            "Step 183700: Loss = 0.039398\n",
            "Step 183700: Loss = 0.039997\n",
            "Step 183800: Loss = 0.036874\n",
            "Step 183800: Loss = 0.041439\n",
            "Step 183900: Loss = 0.040978\n",
            "Step 183900: Loss = 0.041815\n",
            "Step 184000: Loss = 0.035935\n",
            "Step 184000: Loss = 0.037006\n",
            "\n",
            "Saving samples at step 184000\n",
            "Average loss over last 1000 steps: 0.041261\n",
            "Step 184100: Loss = 0.038964\n",
            "Step 184100: Loss = 0.042222\n",
            "Step 184200: Loss = 0.045823\n",
            "Step 184200: Loss = 0.045420\n",
            "Step 184300: Loss = 0.041456\n",
            "Step 184300: Loss = 0.042188\n",
            "Step 184400: Loss = 0.033365\n",
            "Step 184400: Loss = 0.037330\n",
            "Step 184500: Loss = 0.042907\n",
            "Step 184500: Loss = 0.043483\n",
            "Step 184600: Loss = 0.039015\n",
            "Step 184600: Loss = 0.042685\n",
            "Step 184700: Loss = 0.046045\n",
            "Step 184700: Loss = 0.044961\n",
            "Step 184800: Loss = 0.043992\n",
            "Step 184800: Loss = 0.043453\n",
            "Step 184900: Loss = 0.042880\n",
            "Step 184900: Loss = 0.041194\n",
            "Step 185000: Loss = 0.042285\n",
            "Step 185000: Loss = 0.039545\n",
            "\n",
            "Saving samples at step 185000\n",
            "Average loss over last 1000 steps: 0.041181\n",
            "Step 185100: Loss = 0.042521\n",
            "Step 185100: Loss = 0.038242\n",
            "Step 185200: Loss = 0.036003\n",
            "Step 185200: Loss = 0.037587\n",
            "Step 185300: Loss = 0.038300\n",
            "Step 185300: Loss = 0.040963\n",
            "Step 185400: Loss = 0.045188\n",
            "Step 185400: Loss = 0.035347\n",
            "Step 185500: Loss = 0.043503\n",
            "Step 185500: Loss = 0.039501\n",
            "Step 185600: Loss = 0.040816\n",
            "Step 185600: Loss = 0.034333\n",
            "Step 185700: Loss = 0.038835\n",
            "Step 185700: Loss = 0.045241\n",
            "Step 185800: Loss = 0.044743\n",
            "Step 185800: Loss = 0.038735\n",
            "Step 185900: Loss = 0.045114\n",
            "Step 185900: Loss = 0.040771\n",
            "Step 186000: Loss = 0.040848\n",
            "Step 186000: Loss = 0.038365\n",
            "\n",
            "Saving samples at step 186000\n",
            "Average loss over last 1000 steps: 0.041198\n",
            "Step 186100: Loss = 0.041313\n",
            "Step 186100: Loss = 0.034746\n",
            "Step 186200: Loss = 0.047859\n",
            "Step 186200: Loss = 0.037348\n",
            "Step 186300: Loss = 0.036616\n",
            "Step 186300: Loss = 0.035738\n",
            "Step 186400: Loss = 0.039329\n",
            "Step 186400: Loss = 0.040211\n",
            "Step 186500: Loss = 0.040875\n",
            "Step 186500: Loss = 0.040656\n",
            "Step 186600: Loss = 0.043019\n",
            "Step 186600: Loss = 0.044516\n",
            "Step 186700: Loss = 0.048259\n",
            "Step 186700: Loss = 0.038776\n",
            "Step 186800: Loss = 0.039629\n",
            "Step 186800: Loss = 0.040807\n",
            "Step 186900: Loss = 0.041737\n",
            "Step 186900: Loss = 0.035680\n",
            "Step 187000: Loss = 0.045375\n",
            "Step 187000: Loss = 0.042595\n",
            "\n",
            "Saving samples at step 187000\n",
            "Average loss over last 1000 steps: 0.041032\n",
            "Step 187100: Loss = 0.040699\n",
            "Step 187100: Loss = 0.036222\n",
            "Step 187200: Loss = 0.038998\n",
            "Step 187200: Loss = 0.036927\n",
            "Step 187300: Loss = 0.041996\n",
            "Step 187300: Loss = 0.039259\n",
            "Step 187400: Loss = 0.038352\n",
            "Step 187400: Loss = 0.039954\n",
            "Step 187500: Loss = 0.042620\n",
            "Step 187500: Loss = 0.041663\n",
            "Step 187600: Loss = 0.045589\n",
            "Step 187600: Loss = 0.038435\n",
            "Step 187700: Loss = 0.038721\n",
            "Step 187700: Loss = 0.041116\n",
            "Step 187800: Loss = 0.040123\n",
            "Step 187800: Loss = 0.045771\n",
            "Step 187900: Loss = 0.042824\n",
            "Step 187900: Loss = 0.040081\n",
            "Step 188000: Loss = 0.040657\n",
            "Step 188000: Loss = 0.031799\n",
            "\n",
            "Saving samples at step 188000\n",
            "Average loss over last 1000 steps: 0.040990\n",
            "Step 188100: Loss = 0.050204\n",
            "Step 188100: Loss = 0.040182\n",
            "Step 188200: Loss = 0.042396\n",
            "Step 188200: Loss = 0.037911\n",
            "Step 188300: Loss = 0.046898\n",
            "Step 188300: Loss = 0.046263\n",
            "Step 188400: Loss = 0.039102\n",
            "Step 188400: Loss = 0.045763\n",
            "Step 188500: Loss = 0.044366\n",
            "Step 188500: Loss = 0.035142\n",
            "Step 188600: Loss = 0.038328\n",
            "Step 188600: Loss = 0.040204\n",
            "Step 188700: Loss = 0.044354\n",
            "Step 188700: Loss = 0.036135\n",
            "Step 188800: Loss = 0.036110\n",
            "Step 188800: Loss = 0.042086\n",
            "Step 188900: Loss = 0.037548\n",
            "Step 188900: Loss = 0.049273\n",
            "Step 189000: Loss = 0.043034\n",
            "Step 189000: Loss = 0.036014\n",
            "\n",
            "Saving samples at step 189000\n",
            "Average loss over last 1000 steps: 0.040800\n",
            "Step 189100: Loss = 0.043155\n",
            "Step 189100: Loss = 0.044447\n",
            "Step 189200: Loss = 0.043672\n",
            "Step 189200: Loss = 0.042558\n",
            "Step 189300: Loss = 0.039642\n",
            "Step 189300: Loss = 0.046357\n",
            "Step 189400: Loss = 0.035448\n",
            "Step 189400: Loss = 0.043324\n",
            "Step 189500: Loss = 0.040361\n",
            "Step 189500: Loss = 0.039635\n",
            "Step 189600: Loss = 0.041781\n",
            "Step 189600: Loss = 0.040241\n",
            "Step 189700: Loss = 0.040175\n",
            "Step 189700: Loss = 0.040271\n",
            "Step 189800: Loss = 0.036412\n",
            "Step 189800: Loss = 0.037740\n",
            "Step 189900: Loss = 0.041962\n",
            "Step 189900: Loss = 0.039831\n",
            "Step 190000: Loss = 0.035692\n",
            "Step 190000: Loss = 0.042020\n",
            "\n",
            "Saving samples at step 190000\n",
            "Average loss over last 1000 steps: 0.040679\n",
            "Model saved at step 190000\n",
            "\n",
            "Evaluating model at step 190000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.61it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.44it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.43it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.40it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.63it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation at step 190000:\n",
            "Direct reconstruction: SSIM=0.9172, RMSE=0.0699, FID=138.36\n",
            "Deblurred (Algorithm 2): SSIM=0.8622, RMSE=0.1017, FID=143.73\n",
            "Step 190100: Loss = 0.040381\n",
            "Step 190100: Loss = 0.031687\n",
            "Step 190200: Loss = 0.041423\n",
            "Step 190200: Loss = 0.042270\n",
            "Step 190300: Loss = 0.041293\n",
            "Step 190300: Loss = 0.037788\n",
            "Step 190400: Loss = 0.042536\n",
            "Step 190400: Loss = 0.039511\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 190500: Loss = 0.040644\n",
            "Step 190500: Loss = 0.042727\n",
            "Step 190600: Loss = 0.037635\n",
            "Step 190600: Loss = 0.040672\n",
            "Step 190700: Loss = 0.037422\n",
            "Step 190700: Loss = 0.047592\n",
            "Step 190800: Loss = 0.038141\n",
            "Step 190800: Loss = 0.038794\n",
            "Step 190900: Loss = 0.041040\n",
            "Step 190900: Loss = 0.039700\n",
            "Step 191000: Loss = 0.042775\n",
            "Step 191000: Loss = 0.040733\n",
            "\n",
            "Saving samples at step 191000\n",
            "Average loss over last 1000 steps: 0.040556\n",
            "Step 191100: Loss = 0.037466\n",
            "Step 191100: Loss = 0.035183\n",
            "Step 191200: Loss = 0.043208\n",
            "Step 191200: Loss = 0.037726\n",
            "Step 191300: Loss = 0.037024\n",
            "Step 191300: Loss = 0.038495\n",
            "Step 191400: Loss = 0.039682\n",
            "Step 191400: Loss = 0.038828\n",
            "Step 191500: Loss = 0.046736\n",
            "Step 191500: Loss = 0.040643\n",
            "Step 191600: Loss = 0.044563\n",
            "Step 191600: Loss = 0.044807\n",
            "Step 191700: Loss = 0.036635\n",
            "Step 191700: Loss = 0.045717\n",
            "Step 191800: Loss = 0.040917\n",
            "Step 191800: Loss = 0.043493\n",
            "Step 191900: Loss = 0.041019\n",
            "Step 191900: Loss = 0.040742\n",
            "Step 192000: Loss = 0.041295\n",
            "Step 192000: Loss = 0.043194\n",
            "\n",
            "Saving samples at step 192000\n",
            "Average loss over last 1000 steps: 0.040481\n",
            "Step 192100: Loss = 0.042011\n",
            "Step 192100: Loss = 0.045075\n",
            "Step 192200: Loss = 0.040468\n",
            "Step 192200: Loss = 0.041026\n",
            "Step 192300: Loss = 0.038846\n",
            "Step 192300: Loss = 0.039365\n",
            "Step 192400: Loss = 0.040160\n",
            "Step 192400: Loss = 0.035081\n",
            "Step 192500: Loss = 0.040072\n",
            "Step 192500: Loss = 0.039588\n",
            "Step 192600: Loss = 0.041257\n",
            "Step 192600: Loss = 0.045506\n",
            "Step 192700: Loss = 0.037074\n",
            "Step 192700: Loss = 0.044139\n",
            "Step 192800: Loss = 0.042708\n",
            "Step 192800: Loss = 0.043424\n",
            "Step 192900: Loss = 0.043508\n",
            "Step 192900: Loss = 0.043590\n",
            "Step 193000: Loss = 0.044515\n",
            "Step 193000: Loss = 0.034061\n",
            "\n",
            "Saving samples at step 193000\n",
            "Average loss over last 1000 steps: 0.040391\n",
            "Step 193100: Loss = 0.039771\n",
            "Step 193100: Loss = 0.036155\n",
            "Step 193200: Loss = 0.043164\n",
            "Step 193200: Loss = 0.037125\n",
            "Step 193300: Loss = 0.042241\n",
            "Step 193300: Loss = 0.040444\n",
            "Step 193400: Loss = 0.041935\n",
            "Step 193400: Loss = 0.041873\n",
            "Step 193500: Loss = 0.033097\n",
            "Step 193500: Loss = 0.043705\n",
            "Step 193600: Loss = 0.043914\n",
            "Step 193600: Loss = 0.035746\n",
            "Step 193700: Loss = 0.033518\n",
            "Step 193700: Loss = 0.045404\n",
            "Step 193800: Loss = 0.041404\n",
            "Step 193800: Loss = 0.040659\n",
            "Step 193900: Loss = 0.039663\n",
            "Step 193900: Loss = 0.040390\n",
            "Step 194000: Loss = 0.040891\n",
            "Step 194000: Loss = 0.039489\n",
            "\n",
            "Saving samples at step 194000\n",
            "Average loss over last 1000 steps: 0.040299\n",
            "Step 194100: Loss = 0.040119\n",
            "Step 194100: Loss = 0.042683\n",
            "Step 194200: Loss = 0.039229\n",
            "Step 194200: Loss = 0.040549\n",
            "Step 194300: Loss = 0.039022\n",
            "Step 194300: Loss = 0.042314\n",
            "Step 194400: Loss = 0.034538\n",
            "Step 194400: Loss = 0.041442\n",
            "Step 194500: Loss = 0.041102\n",
            "Step 194500: Loss = 0.037960\n",
            "Step 194600: Loss = 0.038776\n",
            "Step 194600: Loss = 0.040509\n",
            "Step 194700: Loss = 0.044444\n",
            "Step 194700: Loss = 0.038466\n",
            "Step 194800: Loss = 0.037586\n",
            "Step 194800: Loss = 0.042904\n",
            "Step 194900: Loss = 0.036640\n",
            "Step 194900: Loss = 0.042565\n",
            "Step 195000: Loss = 0.041556\n",
            "Step 195000: Loss = 0.041879\n",
            "\n",
            "Saving samples at step 195000\n",
            "Average loss over last 1000 steps: 0.040344\n",
            "Step 195100: Loss = 0.040926\n",
            "Step 195100: Loss = 0.039559\n",
            "Step 195200: Loss = 0.037419\n",
            "Step 195200: Loss = 0.040184\n",
            "Step 195300: Loss = 0.044813\n",
            "Step 195300: Loss = 0.043790\n",
            "Step 195400: Loss = 0.039796\n",
            "Step 195400: Loss = 0.038413\n",
            "Step 195500: Loss = 0.042370\n",
            "Step 195500: Loss = 0.043189\n",
            "Step 195600: Loss = 0.042101\n",
            "Step 195600: Loss = 0.041931\n",
            "Step 195700: Loss = 0.033773\n",
            "Step 195700: Loss = 0.040655\n",
            "Step 195800: Loss = 0.038925\n",
            "Step 195800: Loss = 0.038324\n",
            "Step 195900: Loss = 0.044447\n",
            "Step 195900: Loss = 0.040732\n",
            "Step 196000: Loss = 0.039194\n",
            "Step 196000: Loss = 0.044532\n",
            "\n",
            "Saving samples at step 196000\n",
            "Average loss over last 1000 steps: 0.040217\n",
            "Step 196100: Loss = 0.036438\n",
            "Step 196100: Loss = 0.035956\n",
            "Step 196200: Loss = 0.034239\n",
            "Step 196200: Loss = 0.039928\n",
            "Step 196300: Loss = 0.043658\n",
            "Step 196300: Loss = 0.034978\n",
            "Step 196400: Loss = 0.043150\n",
            "Step 196400: Loss = 0.040730\n",
            "Step 196500: Loss = 0.041405\n",
            "Step 196500: Loss = 0.040687\n",
            "Step 196600: Loss = 0.030393\n",
            "Step 196600: Loss = 0.042325\n",
            "Step 196700: Loss = 0.046597\n",
            "Step 196700: Loss = 0.040891\n",
            "Step 196800: Loss = 0.041738\n",
            "Step 196800: Loss = 0.040925\n",
            "Step 196900: Loss = 0.039960\n",
            "Step 196900: Loss = 0.038406\n",
            "Step 197000: Loss = 0.036925\n",
            "Step 197000: Loss = 0.038482\n",
            "\n",
            "Saving samples at step 197000\n",
            "Average loss over last 1000 steps: 0.039957\n",
            "Step 197100: Loss = 0.036340\n",
            "Step 197100: Loss = 0.041749\n",
            "Step 197200: Loss = 0.038805\n",
            "Step 197200: Loss = 0.035310\n",
            "Step 197300: Loss = 0.037235\n",
            "Step 197300: Loss = 0.035028\n",
            "Step 197400: Loss = 0.039250\n",
            "Step 197400: Loss = 0.041947\n",
            "Step 197500: Loss = 0.042438\n",
            "Step 197500: Loss = 0.039458\n",
            "Step 197600: Loss = 0.043173\n",
            "Step 197600: Loss = 0.044130\n",
            "Step 197700: Loss = 0.040325\n",
            "Step 197700: Loss = 0.039595\n",
            "Step 197800: Loss = 0.036611\n",
            "Step 197800: Loss = 0.043117\n",
            "Step 197900: Loss = 0.041492\n",
            "Step 197900: Loss = 0.042480\n",
            "Step 198000: Loss = 0.037381\n",
            "Step 198000: Loss = 0.043530\n",
            "\n",
            "Saving samples at step 198000\n",
            "Average loss over last 1000 steps: 0.039922\n",
            "Step 198100: Loss = 0.040682\n",
            "Step 198100: Loss = 0.042105\n",
            "Step 198200: Loss = 0.039668\n",
            "Step 198200: Loss = 0.040913\n",
            "Step 198300: Loss = 0.040196\n",
            "Step 198300: Loss = 0.036905\n",
            "Step 198400: Loss = 0.033090\n",
            "Step 198400: Loss = 0.038170\n",
            "Step 198500: Loss = 0.045268\n",
            "Step 198500: Loss = 0.037020\n",
            "Step 198600: Loss = 0.037012\n",
            "Step 198600: Loss = 0.039852\n",
            "Step 198700: Loss = 0.042039\n",
            "Step 198700: Loss = 0.040877\n",
            "Step 198800: Loss = 0.040423\n",
            "Step 198800: Loss = 0.042097\n",
            "Step 198900: Loss = 0.036824\n",
            "Step 198900: Loss = 0.041699\n",
            "Step 199000: Loss = 0.034926\n",
            "Step 199000: Loss = 0.039391\n",
            "\n",
            "Saving samples at step 199000\n",
            "Average loss over last 1000 steps: 0.039839\n",
            "Step 199100: Loss = 0.040267\n",
            "Step 199100: Loss = 0.048098\n",
            "Step 199200: Loss = 0.039781\n",
            "Step 199200: Loss = 0.038884\n",
            "Step 199300: Loss = 0.044343\n",
            "Step 199300: Loss = 0.038856\n",
            "Step 199400: Loss = 0.037082\n",
            "Step 199400: Loss = 0.042253\n",
            "Step 199500: Loss = 0.039178\n",
            "Step 199500: Loss = 0.036321\n",
            "Step 199600: Loss = 0.043226\n",
            "Step 199600: Loss = 0.042162\n",
            "Step 199700: Loss = 0.038895\n",
            "Step 199700: Loss = 0.036362\n",
            "Step 199800: Loss = 0.034221\n",
            "Step 199800: Loss = 0.035063\n",
            "Step 199900: Loss = 0.043784\n",
            "Step 199900: Loss = 0.039469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.93it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.92it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.65it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.86it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.41it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation at step 200000:\n",
            "Direct reconstruction: SSIM=0.9211, RMSE=0.0678, FID=136.12\n",
            "Deblurred (Algorithm 2): SSIM=0.8675, RMSE=0.0991, FID=144.40\n",
            "Training completed!\n",
            "Evaluating model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00,  3.76it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.54it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.32it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.56it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.36it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation at step 200000:\n",
            "Direct reconstruction: SSIM=0.9211, RMSE=0.0678, FID=136.12\n",
            "Deblurred (Algorithm 2): SSIM=0.8675, RMSE=0.0991, FID=144.62\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "from torch.utils import data\n",
        "from pathlib import Path\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "import pytorch_fid\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "res_dir = '/content/drive/MyDrive/Project'\n",
        "\n",
        "# helper funcs\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    grps = num // divisor\n",
        "    leftover = num % divisor\n",
        "    arr = [divisor] * grps\n",
        "    if leftover > 0:\n",
        "        arr.append(leftover)\n",
        "    return arr\n",
        "\n",
        "# small helpers\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old, new = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old, new)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        dev = x.device\n",
        "        half_d = self.dim // 2\n",
        "        emb = math.log(10000) / (half_d - 1)\n",
        "        emb = torch.exp(torch.arange(half_d, device=dev) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
        "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
        "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "    # from that paper\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim = None, mult = 2, norm = True):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_emb_dim, dim)\n",
        "        ) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding = 3, groups = dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            LayerNorm(dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding = 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding = 1)\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp):\n",
        "            assert exists(time_emb), 'time emb must be passed in'\n",
        "            cond = self.mlp(time_emb)\n",
        "            h = h + rearrange(cond, 'b c -> b c 1 1')\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "\n",
        "        k = k.softmax(dim = -1)\n",
        "        ctx = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = torch.einsum('b h d e, b h d n -> b h e n', ctx, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels = 3,\n",
        "        with_time_emb = True,\n",
        "        residual = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.residual = residual\n",
        "        print(\"Is Time embed used ? \", with_time_emb)\n",
        "\n",
        "        dims = [channels, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if with_time_emb:\n",
        "            time_dim = dim\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPosEmb(dim),\n",
        "                nn.Linear(dim, dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim * 4, dim)\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_in, dim_out, time_emb_dim = time_dim, norm = ind != 0),\n",
        "                ConvNextBlock(dim_out, dim_out, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                Downsample(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
        "        self.mid_block2 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_out * 2, dim_in, time_emb_dim = time_dim),\n",
        "                ConvNextBlock(dim_in, dim_in, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                Upsample(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            ConvNextBlock(dim, dim),\n",
        "            nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        orig_x = x\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for convnext, convnext2, attn, downsample in self.downs:\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for convnext, convnext2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        if self.residual:\n",
        "            return self.final_conv(x) + orig_x\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "# gaussian diffusion trainer class - from the second code\n",
        "class GaussianDiffusion(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        denoise_fn,\n",
        "        *,\n",
        "        image_size,\n",
        "        device_of_kernel,\n",
        "        channels = 3,\n",
        "        timesteps = 1000,\n",
        "        loss_type = 'l1',\n",
        "        kernel_std = 0.1,\n",
        "        kernel_size = 3,\n",
        "        blur_routine = 'Incremental',\n",
        "        train_routine = 'Final',\n",
        "        sampling_routine='default',\n",
        "        discrete=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.image_size = image_size\n",
        "        self.denoise_fn = denoise_fn\n",
        "        self.device_of_kernel = device_of_kernel\n",
        "\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        self.loss_type = loss_type\n",
        "        self.kernel_std = kernel_std\n",
        "        self.kernel_size = kernel_size\n",
        "        self.blur_routine = blur_routine\n",
        "\n",
        "        # Create Gaussian kernels for each timestep\n",
        "        self.gaussian_kernels = nn.ModuleList(self.get_kernels())\n",
        "        self.train_routine = train_routine\n",
        "        self.sampling_routine = sampling_routine\n",
        "        self.discrete = discrete\n",
        "\n",
        "    def blur(self, dims, std):\n",
        "        \"\"\"Creates a Gaussian blur kernel with the given dimensions and standard deviation\"\"\"\n",
        "        kernel = self._create_gaussian_kernel(dims, std)\n",
        "        return kernel\n",
        "\n",
        "    def get_kernels(self):\n",
        "        \"\"\"Creates all Gaussian blur kernels for all timesteps\"\"\"\n",
        "        kernels = []\n",
        "        for i in range(self.num_timesteps):\n",
        "            if self.blur_routine == 'Incremental':\n",
        "                kstd = self.kernel_std*(i+1)\n",
        "                kernels.append(self.get_conv(self.kernel_size, kstd))\n",
        "            elif self.blur_routine == 'Constant':\n",
        "                # For MNIST: fixed standard deviation\n",
        "                kernels.append(self.get_conv(self.kernel_size, self.kernel_std))\n",
        "            elif self.blur_routine == 'Exponential':\n",
        "                kstd = np.exp(self.kernel_std * i)\n",
        "                kernels.append(self.get_conv(self.kernel_size, kstd))\n",
        "            elif self.blur_routine == 'CIFAR':\n",
        "                # For CIFAR-10: 0.01*t + 0.35\n",
        "                kstd = 0.01 * i + 0.35\n",
        "                kernels.append(self.get_conv(self.kernel_size, kstd))\n",
        "        return kernels\n",
        "\n",
        "    def get_conv(self, kernel_size, std, mode='circular'):\n",
        "        \"\"\"Creates a convolution with Gaussian kernel\"\"\"\n",
        "        kernel = self._create_gaussian_kernel(kernel_size, std)\n",
        "        conv = nn.Conv2d(in_channels=self.channels, out_channels=self.channels,\n",
        "                        kernel_size=kernel_size, padding=int((kernel_size-1)/2), padding_mode=mode,\n",
        "                        bias=False, groups=self.channels)\n",
        "        with torch.no_grad():\n",
        "            # Ensure kernel has the right shape before repeating\n",
        "            if kernel.dim() == 2:\n",
        "                kernel = kernel.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            # Now repeat for each channel\n",
        "            kernel = kernel.repeat(self.channels, 1, 1, 1)\n",
        "            conv.weight = nn.Parameter(kernel)\n",
        "\n",
        "        return conv\n",
        "\n",
        "    def _create_gaussian_kernel(self, kernel_size, std):\n",
        "        \"\"\"Create a 2D Gaussian kernel manually\"\"\"\n",
        "        x = torch.arange(-(kernel_size // 2), kernel_size // 2 + 1, 1).to(self.device_of_kernel)\n",
        "        y = torch.arange(-(kernel_size // 2), kernel_size // 2 + 1, 1).to(self.device_of_kernel)\n",
        "        xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
        "        kernel = torch.exp(-(xx**2 + yy**2) / (2 * std**2))\n",
        "        kernel = kernel / kernel.sum()\n",
        "        return kernel\n",
        "\n",
        "    def q_sample(self, x_start, t):\n",
        "        \"\"\"Forward process: progressively blur the image based on timestep t\"\"\"\n",
        "        max_iters = torch.max(t)\n",
        "        all_blurs = []\n",
        "        x = x_start\n",
        "        for i in range(max_iters+1):\n",
        "            with torch.no_grad():\n",
        "                x = self.gaussian_kernels[i](x)\n",
        "                if self.discrete:\n",
        "                    if i == (self.num_timesteps-1):\n",
        "                        x = torch.mean(x, [2, 3], keepdim=True)\n",
        "                        x = x.expand(x_start.shape[0], x_start.shape[1], x_start.shape[2], x_start.shape[3])\n",
        "                all_blurs.append(x)\n",
        "\n",
        "        all_blurs = torch.stack(all_blurs)\n",
        "\n",
        "        # Select the appropriate blur level for each sample in the batch\n",
        "        choose_blur = []\n",
        "        for step in range(t.shape[0]):\n",
        "            if step != -1:\n",
        "                choose_blur.append(all_blurs[t[step], step])\n",
        "            else:\n",
        "                choose_blur.append(x_start[step])\n",
        "\n",
        "        choose_blur = torch.stack(choose_blur)\n",
        "        return choose_blur\n",
        "\n",
        "    def p_losses(self, x_start, t):\n",
        "        \"\"\"Calculate training loss\"\"\"\n",
        "        b, c, h, w = x_start.shape\n",
        "        if self.train_routine == 'Final':\n",
        "            x_blur = self.q_sample(x_start=x_start, t=t)\n",
        "            x_recon = self.denoise_fn(x_blur, t)\n",
        "            if self.loss_type == 'l1':\n",
        "                loss = (x_start - x_recon).abs().mean()\n",
        "            elif self.loss_type == 'l2':\n",
        "                loss = F.mse_loss(x_start, x_recon)\n",
        "            else:\n",
        "                raise NotImplementedError()\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"Forward pass during training\"\"\"\n",
        "        b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
        "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "        return self.p_losses(x, t, *args, **kwargs)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size=16, img=None, t=None):\n",
        "        \"\"\"Sampling algorithm - Algorithm 2 from the paper\"\"\"\n",
        "        self.denoise_fn.eval()\n",
        "\n",
        "        if t is None:\n",
        "            t = self.num_timesteps\n",
        "\n",
        "        # Apply forward process (blur)\n",
        "        if self.blur_routine == 'Individual_Incremental':\n",
        "            img = self.gaussian_kernels[t-1](img)\n",
        "        else:\n",
        "            for i in range(t):\n",
        "                with torch.no_grad():\n",
        "                    img = self.gaussian_kernels[i](img)\n",
        "\n",
        "        # Store blurred input for later\n",
        "        xt = img.clone()\n",
        "        direct_recons = None\n",
        "\n",
        "        # Iterative deblurring (reverse process)\n",
        "        while(t):\n",
        "            step = torch.full((batch_size,), t - 1, dtype=torch.long).to(img.device)\n",
        "            x0_pred = self.denoise_fn(img, step)  # Predict clean image\n",
        "\n",
        "            if self.train_routine == 'Final':\n",
        "                if direct_recons is None:\n",
        "                    direct_recons = x0_pred\n",
        "\n",
        "                if self.sampling_routine == 'x0_step_down':\n",
        "                    # Algorithm 2 from the paper\n",
        "                    x_times = x0_pred.clone()\n",
        "                    for i in range(t):\n",
        "                        with torch.no_grad():\n",
        "                            x_times = self.gaussian_kernels[i](x_times)\n",
        "                            if self.discrete:\n",
        "                                if i == (self.num_timesteps - 1):\n",
        "                                    x_times = torch.mean(x_times, [2, 3], keepdim=True)\n",
        "                                    x_times = x_times.expand(x_times.shape[0], x_times.shape[1],\n",
        "                                                             x_times.shape[2], x_times.shape[3])\n",
        "\n",
        "                    x_times_sub_1 = x0_pred.clone()\n",
        "                    for i in range(t - 1):\n",
        "                        with torch.no_grad():\n",
        "                            x_times_sub_1 = self.gaussian_kernels[i](x_times_sub_1)\n",
        "\n",
        "                    # Key step in Algorithm 2\n",
        "                    img = img - x_times + x_times_sub_1\n",
        "\n",
        "                elif self.sampling_routine == 'default':\n",
        "                    # Algorithm 1 - for comparison\n",
        "                    if self.blur_routine == 'Individual_Incremental':\n",
        "                        img = self.gaussian_kernels[t - 2](x0_pred) if t > 1 else x0_pred\n",
        "                    else:\n",
        "                        img = x0_pred.clone()\n",
        "                        for i in range(t-1):\n",
        "                            with torch.no_grad():\n",
        "                                img = self.gaussian_kernels[i](img)\n",
        "\n",
        "            t = t - 1\n",
        "\n",
        "        self.denoise_fn.train()\n",
        "        return xt, direct_recons, img\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, folder, image_size, exts=['jpg', 'jpeg', 'png']):\n",
        "        super().__init__()\n",
        "        self.folder = folder\n",
        "        self.image_size = image_size\n",
        "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((int(image_size*1.12), int(image_size*1.12))),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)  # Scale to [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n",
        "\n",
        "def calculate_metrics(orig_imgs, recon_imgs):\n",
        "    \"\"\"metrics calc - ssim/rmse/fid\"\"\"\n",
        "    # convert to numpy\n",
        "    orig_np = (orig_imgs.cpu().permute(0, 2, 3, 1) + 1) / 2\n",
        "    recon_np = (recon_imgs.cpu().permute(0, 2, 3, 1) + 1) / 2\n",
        "\n",
        "    # ssim\n",
        "    ssim_vals = []\n",
        "    for i in range(orig_np.shape[0]):\n",
        "        orig = orig_np[i].numpy()\n",
        "        recon = recon_np[i].numpy()\n",
        "\n",
        "        if orig.shape[2] == 1:\n",
        "            orig = orig.squeeze(2)\n",
        "            recon = recon.squeeze(2)\n",
        "            ssim_val = ssim(orig, recon, data_range=1.0, win_size=5, channel_axis=None)\n",
        "        else:\n",
        "            ssim_val = ssim(orig, recon, data_range=1.0, win_size=5, channel_axis=2)\n",
        "\n",
        "        ssim_vals.append(ssim_val)\n",
        "\n",
        "    avg_ssim = np.mean(ssim_vals)\n",
        "\n",
        "    # rmse\n",
        "    mse = F.mse_loss(orig_imgs, recon_imgs).item()\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # fid stuff\n",
        "    tmp_orig = 'temp_orig'\n",
        "    tmp_recon = 'temp_recon'\n",
        "    os.makedirs(tmp_orig, exist_ok=True)\n",
        "    os.makedirs(tmp_recon, exist_ok=True)\n",
        "\n",
        "    # save images for fid\n",
        "    for i in range(min(orig_imgs.shape[0], 100)):\n",
        "        utils.save_image((orig_imgs[i] + 1) / 2, os.path.join(tmp_orig, f'{i}.png'))\n",
        "        utils.save_image((recon_imgs[i] + 1) / 2, os.path.join(tmp_recon, f'{i}.png'))\n",
        "\n",
        "    # calc fid\n",
        "    fid = fid_score.calculate_fid_given_paths([tmp_orig, tmp_recon],\n",
        "                                   batch_size=50, device=orig_imgs.device,\n",
        "                                   dims=2048)\n",
        "    # cleanup\n",
        "    shutil.rmtree(tmp_orig)\n",
        "    shutil.rmtree(tmp_recon)\n",
        "\n",
        "    return avg_ssim, rmse, fid\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        dataset_type,\n",
        "        *,\n",
        "        ema_decay = 0.995,\n",
        "        image_size = 32,\n",
        "        train_batch_size = 32,\n",
        "        eval_batch_size = 16,\n",
        "        train_lr = 2e-5,\n",
        "        train_num_steps = 100000,\n",
        "        gradient_accumulate_every = 2,\n",
        "        step_start_ema = 2000,\n",
        "        update_ema_every = 10,\n",
        "        save_and_sample_every = 1000,\n",
        "        results_folder = './results',\n",
        "        load_path = None,\n",
        "        eval_dataset = None,\n",
        "        save_model_every = 10000,\n",
        "        eval_every = 10000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = diffusion_model\n",
        "        self.ema = EMA(ema_decay)\n",
        "        self.ema_model = copy.deepcopy(self.model)\n",
        "        self.update_ema_every = update_ema_every\n",
        "\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "        self.save_model_every = save_model_every\n",
        "        self.eval_every = eval_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.eval_batch_size = eval_batch_size\n",
        "        self.image_size = image_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "        self.train_num_steps = train_num_steps\n",
        "\n",
        "        self.dataset_type = dataset_type\n",
        "        self.setup_dataset(dataset_type)\n",
        "\n",
        "        self.eval_dataset = eval_dataset\n",
        "        if eval_dataset is None:\n",
        "            # use training subset\n",
        "            eval_size = min(1000, len(self.ds))\n",
        "            eval_indices = np.random.choice(len(self.ds), eval_size, replace=False)\n",
        "            self.eval_dataset = torch.utils.data.Subset(self.ds, eval_indices)\n",
        "\n",
        "        self.eval_dl = data.DataLoader(self.eval_dataset, batch_size=eval_batch_size,\n",
        "                                    shuffle=False, drop_last=False)\n",
        "\n",
        "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr)\n",
        "        self.step = 0\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # metrics file\n",
        "        self.metrics_file = self.results_folder / 'metrics.csv'\n",
        "        if not self.metrics_file.exists():\n",
        "            with open(self.metrics_file, 'w') as f:\n",
        "                f.write('step,ssim,rmse,fid\\n')\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if load_path is not None:\n",
        "            self.load(load_path)\n",
        "\n",
        "    def setup_dataset(self, dataset_type):\n",
        "        if dataset_type == 'mnist':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Pad(2),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "            ])\n",
        "\n",
        "            # get mnist\n",
        "            self.ds = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=self.batch_size, shuffle=True,\n",
        "                                        pin_memory=True, num_workers=8, drop_last=True))\n",
        "\n",
        "        elif dataset_type == 'cifar10':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "            ])\n",
        "\n",
        "            # get cifar\n",
        "            self.ds = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=self.batch_size, shuffle=True,\n",
        "                                        pin_memory=True, num_workers=8, drop_last=True))\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.ema_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def step_ema(self):\n",
        "        if self.step < self.step_start_ema:\n",
        "            self.reset_parameters()\n",
        "            return\n",
        "        self.ema.update_model_average(self.ema_model, self.model)\n",
        "\n",
        "    def save(self, milestone=None):\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.model.state_dict(),\n",
        "            'ema': self.ema_model.state_dict()\n",
        "        }\n",
        "        if milestone is None:\n",
        "            torch.save(data, str(self.results_folder / f'model.pt'))\n",
        "        else:\n",
        "            torch.save(data, str(self.results_folder / f'model_{milestone}.pt'))\n",
        "\n",
        "    def load(self, load_path):\n",
        "        print(f\"Loading model from {load_path}\")\n",
        "        data = torch.load(load_path)\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.model.load_state_dict(data['model'])\n",
        "        self.ema_model.load_state_dict(data['ema'])\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"eval on test set\"\"\"\n",
        "        self.ema_model.eval()\n",
        "\n",
        "        all_orig_imgs = []\n",
        "        all_blurred_imgs = []\n",
        "        all_direct_recons = []\n",
        "        all_deblurred_imgs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.eval_dl:\n",
        "                if isinstance(batch, (list, tuple)):\n",
        "                    # handle (img, label)\n",
        "                    orig_imgs = batch[0].to(self.model.device_of_kernel)\n",
        "                else:\n",
        "                    # handle just img\n",
        "                    orig_imgs = batch.to(self.model.device_of_kernel)\n",
        "\n",
        "                # use model for deblur\n",
        "                blurred_imgs, direct_recons, deblurred_imgs = self.ema_model.sample(\n",
        "                    batch_size=orig_imgs.shape[0], img=orig_imgs)\n",
        "\n",
        "                all_orig_imgs.append(orig_imgs)\n",
        "                all_blurred_imgs.append(blurred_imgs)\n",
        "                all_direct_recons.append(direct_recons)\n",
        "                all_deblurred_imgs.append(deblurred_imgs)\n",
        "\n",
        "                # limit to 100 imgs\n",
        "                if len(all_orig_imgs) * self.eval_batch_size >= 100:\n",
        "                    break\n",
        "\n",
        "        # cat everything\n",
        "        orig_imgs = torch.cat(all_orig_imgs, dim=0)\n",
        "        blurred_imgs = torch.cat(all_blurred_imgs, dim=0)\n",
        "        direct_recons = torch.cat(all_direct_recons, dim=0)\n",
        "        deblurred_imgs = torch.cat(all_deblurred_imgs, dim=0)\n",
        "\n",
        "        # direct recon metrics\n",
        "        direct_ssim, direct_rmse, direct_fid = calculate_metrics(orig_imgs, direct_recons)\n",
        "\n",
        "        # alg 2 metrics\n",
        "        deblurred_ssim, deblurred_rmse, deblurred_fid = calculate_metrics(orig_imgs, deblurred_imgs)\n",
        "\n",
        "        # baseline metrics\n",
        "        blurred_ssim, blurred_rmse, blurred_fid = calculate_metrics(orig_imgs, blurred_imgs)\n",
        "\n",
        "        # results dict\n",
        "        metrics = {\n",
        "            'direct': {'ssim': direct_ssim, 'rmse': direct_rmse, 'fid': direct_fid},\n",
        "            'deblurred': {'ssim': deblurred_ssim, 'rmse': deblurred_rmse, 'fid': deblurred_fid},\n",
        "            'blurred': {'ssim': blurred_ssim, 'rmse': blurred_rmse, 'fid': blurred_fid}\n",
        "        }\n",
        "\n",
        "        # log to file\n",
        "        with open(self.metrics_file, 'a') as f:\n",
        "            f.write(f\"{self.step},{deblurred_ssim},{deblurred_rmse},{deblurred_fid}\\n\")\n",
        "\n",
        "        # make pictures\n",
        "        vis_folder = self.results_folder / f\"eval_{self.step}\"\n",
        "        vis_folder.mkdir(exist_ok=True)\n",
        "\n",
        "        # save imgs\n",
        "        n_samples = min(8, orig_imgs.shape[0])\n",
        "        samples = torch.stack([\n",
        "            orig_imgs[:n_samples],\n",
        "            blurred_imgs[:n_samples],\n",
        "            direct_recons[:n_samples],\n",
        "            deblurred_imgs[:n_samples]\n",
        "        ])\n",
        "        samples = (samples + 1) / 2  # [0, 1] range\n",
        "\n",
        "        # grid: diff samples in rows\n",
        "        grid = utils.make_grid(samples.view(-1, *samples.shape[2:]), nrow=n_samples)\n",
        "        utils.save_image(grid, vis_folder / 'samples.png')\n",
        "\n",
        "        # plot metrics\n",
        "        if self.step > 0:\n",
        "            self.plot_metrics()\n",
        "\n",
        "        print(f\"Eval at step {self.step}:\")\n",
        "        print(f\"Direct recon: SSIM={direct_ssim:.4f}, RMSE={direct_rmse:.4f}, FID={direct_fid:.2f}\")\n",
        "        print(f\"Deblur (Alg 2): SSIM={deblurred_ssim:.4f}, RMSE={deblurred_rmse:.4f}, FID={deblurred_fid:.2f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"plot stuff over time\"\"\"\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        df = pd.read_csv(self.metrics_file)\n",
        "\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # ssim plot\n",
        "        axs[0].plot(df['step'], df['ssim'])\n",
        "        axs[0].set_title('SSIM over time (higher=better)')\n",
        "        axs[0].set_xlabel('Training steps')\n",
        "        axs[0].set_ylabel('SSIM')\n",
        "\n",
        "        # rmse plot\n",
        "        axs[1].plot(df['step'], df['rmse'])\n",
        "        axs[1].set_title('RMSE over time (lower=better)')\n",
        "        axs[1].set_xlabel('Training steps')\n",
        "        axs[1].set_ylabel('RMSE')\n",
        "\n",
        "        # fid plot\n",
        "        axs[2].plot(df['step'], df['fid'])\n",
        "        axs[2].set_title('FID over time (lower=better)')\n",
        "        axs[2].set_xlabel('Training steps')\n",
        "        axs[2].set_ylabel('FID')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.results_folder / 'metrics_plot.png')\n",
        "        plt.close()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"train the model\"\"\"\n",
        "        device = self.model.device_of_kernel\n",
        "\n",
        "        acc_loss = 0\n",
        "        while self.step < self.train_num_steps:\n",
        "            uloss = 0\n",
        "            for i in range(self.gradient_accumulate_every):\n",
        "                # get batch\n",
        "                if self.dataset_type in ['mnist', 'cifar10']:\n",
        "                    # handle (img, label)\n",
        "                    data, _ = next(self.dl)\n",
        "                    data = data.to(device)\n",
        "                else:\n",
        "                    # handle just imgs\n",
        "                    data = next(self.dl).to(device)\n",
        "\n",
        "                # forward\n",
        "                loss = torch.mean(self.model(data))\n",
        "                if self.step % 100 == 0:\n",
        "                    print(f'Step {self.step}: Loss = {loss.item():.6f}')\n",
        "\n",
        "                uloss += loss.item()\n",
        "\n",
        "                # backward with grad accum\n",
        "                loss = loss / self.gradient_accumulate_every\n",
        "                loss.backward()\n",
        "\n",
        "            # update\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "            # track avg loss\n",
        "            acc_loss = acc_loss + (uloss / self.gradient_accumulate_every)\n",
        "\n",
        "            # ema update\n",
        "            if self.step % self.update_ema_every == 0:\n",
        "                self.step_ema()\n",
        "\n",
        "            # save stuff and metrics\n",
        "            if self.step != 0 and self.step % self.save_and_sample_every == 0:\n",
        "                milestone = self.step // self.save_and_sample_every\n",
        "                print(f\"\\nSaving samples at step {self.step}\")\n",
        "\n",
        "                # get imgs for viz\n",
        "                if self.dataset_type in ['mnist', 'cifar10']:\n",
        "                    viz_batch, _ = next(self.dl)\n",
        "                    viz_batch = viz_batch.to(device)\n",
        "                else:\n",
        "                    viz_batch = next(self.dl).to(device)\n",
        "\n",
        "                # sample w/ ema\n",
        "                with torch.no_grad():\n",
        "                    blurred, direct_recons, deblurred = self.ema_model.sample(\n",
        "                        batch_size=viz_batch.shape[0], img=viz_batch)\n",
        "\n",
        "                # save imgs\n",
        "                viz_folder = self.results_folder / f\"samples_{self.step}\"\n",
        "                viz_folder.mkdir(exist_ok=True)\n",
        "\n",
        "                # [0,1] range for viz\n",
        "                viz_batch = (viz_batch + 1) * 0.5\n",
        "                blurred = (blurred + 1) * 0.5\n",
        "                direct_recons = (direct_recons + 1) * 0.5\n",
        "                deblurred = (deblurred + 1) * 0.5\n",
        "\n",
        "                # grid + save\n",
        "                grid = torch.cat([\n",
        "                    viz_batch[:8],\n",
        "                    blurred[:8],\n",
        "                    direct_recons[:8],\n",
        "                    deblurred[:8]\n",
        "                ])\n",
        "                utils.save_image(grid, viz_folder / 'grid.png', nrow=8)\n",
        "\n",
        "                # individual imgs\n",
        "                utils.save_image(viz_batch[:8], viz_folder / 'original.png', nrow=4)\n",
        "                utils.save_image(blurred[:8], viz_folder / 'blurred.png', nrow=4)\n",
        "                utils.save_image(direct_recons[:8], viz_folder / 'direct_recons.png', nrow=4)\n",
        "                utils.save_image(deblurred[:8], viz_folder / 'deblurred.png', nrow=4)\n",
        "\n",
        "                # print avg loss\n",
        "                avg_loss = acc_loss / (self.save_and_sample_every)\n",
        "                print(f'Avg loss (last {self.save_and_sample_every} steps): {avg_loss:.6f}')\n",
        "                acc_loss = 0\n",
        "\n",
        "            # save checkpoint\n",
        "            if self.step != 0 and self.step % self.save_model_every == 0:\n",
        "                self.save(self.step)\n",
        "                print(f\"Model saved at step {self.step}\")\n",
        "\n",
        "            # eval model\n",
        "            if self.step != 0 and self.step % self.eval_every == 0:\n",
        "                print(f\"\\nEvaluating at step {self.step}\")\n",
        "                self.evaluate()\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "        # final stuff\n",
        "        self.save()\n",
        "        self.evaluate()\n",
        "        print('Training done!')\n",
        "\n",
        "# Run main\n",
        "args = type('Args', (), {\n",
        "    'dataset': 'cifar10',  # mnist or cifar10\n",
        "    'batch_size': 32,\n",
        "    'eval_batch_size': 16,\n",
        "    'epochs': 50,\n",
        "    'save_every': 10000,\n",
        "    'eval_every': 10000,\n",
        "    'sample_every': 1000,\n",
        "    'results_dir': f\"{res_dir}/blur_diffusion\",\n",
        "    'load_model': '/content/drive/MyDrive/Project/blur_diffusion/model_170000.pt',\n",
        "    'train': True,\n",
        "    'eval': True\n",
        "})()\n",
        "\n",
        "# device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# setup config\n",
        "if args.dataset == 'mnist':\n",
        "    channels = 1\n",
        "    image_size = 32\n",
        "    model_dim = 32\n",
        "    timesteps = 40\n",
        "    kernel_size = 11\n",
        "    kernel_std = 7.0  # Fixed std=7 for MNIST\n",
        "    blur_routine = 'Constant'  # Use constant for fixed std\n",
        "elif args.dataset == 'cifar10':\n",
        "    channels = 3\n",
        "    image_size = 32\n",
        "    model_dim = 64\n",
        "    timesteps = 100\n",
        "    kernel_size = 11\n",
        "    kernel_std = 0.01  # For formula: 0.01*t + 0.35\n",
        "    blur_routine = 'CIFAR'  # Custom routine for CIFAR-10\n",
        "\n",
        "# make results dir\n",
        "results_dir = Path(args.results_dir) / f\"blur_{args.dataset}\"\n",
        "results_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# make model\n",
        "model = Unet(\n",
        "    dim=model_dim,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4, 8),\n",
        "    with_time_emb=True\n",
        ").to(device)\n",
        "\n",
        "# make diffusion\n",
        "diffusion = GaussianDiffusion(\n",
        "    model,\n",
        "    image_size=image_size,\n",
        "    device_of_kernel=device,\n",
        "    channels=channels,\n",
        "    timesteps=timesteps,\n",
        "    loss_type='l1',\n",
        "    kernel_std=kernel_std,\n",
        "    kernel_size=kernel_size,\n",
        "    blur_routine=blur_routine,\n",
        "    train_routine='Final',\n",
        "    sampling_routine='x0_step_down'  # Use Algorithm 2 from the paper\n",
        ").to(device)\n",
        "\n",
        "total_steps = 200000\n",
        "\n",
        "# make trainer\n",
        "trainer = Trainer(\n",
        "    diffusion,\n",
        "    args.dataset,\n",
        "    image_size=image_size,\n",
        "    train_batch_size=args.batch_size,\n",
        "    eval_batch_size=args.eval_batch_size,\n",
        "    train_lr=2e-5,\n",
        "    train_num_steps=total_steps,\n",
        "    save_and_sample_every=args.sample_every,\n",
        "    save_model_every=args.save_every,\n",
        "    eval_every=args.eval_every,\n",
        "    results_folder=results_dir,\n",
        "    load_path=args.load_model\n",
        ")\n",
        "\n",
        "if args.train:\n",
        "    print(f\"Starting training: {args.epochs} epochs ({total_steps} steps)...\")\n",
        "    trainer.train()\n",
        "\n",
        "if args.eval:\n",
        "    print(\"Evaluating model...\")\n",
        "    trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}