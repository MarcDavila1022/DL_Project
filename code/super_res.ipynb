{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrQU-SHEuujG",
        "outputId": "f58da8d6-76ed-4a90-98a0-4d94cf8ed3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting torch-fidelity\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.3.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, torch-fidelity\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-fidelity-0.3.0 torchmetrics-1.7.1\n",
            "Collecting pytorch-fid\n",
            "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (1.15.2)\n",
            "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-fid) (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.1->pytorch-fid) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (3.0.2)\n",
            "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pytorch-fid\n",
            "Successfully installed pytorch-fid-0.3.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision torchmetrics torch-fidelity numpy matplotlib scikit-image\n",
        "!pip install pytorch-fid  # For FID calculation\n",
        "!pip install tqdm\n",
        "# Install required packages\n",
        "!pip install -q kornia tqdm einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "rqTEe3i-ur_Q",
        "outputId": "abfa8778-ae06-4d13-db45-07162e63fb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "Is Time embed used ?  True\n",
            "Super Resolution Diffusion: Image size: 32, Min size: 4\n",
            "Number of downsampling steps: 3\n",
            "Scale factors: [0.5, 0.25, 0.16666666666666666]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.8MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/drive/MyDrive/Project/super_cifar10/model_170000.pt\n",
            "Starting training: 50 epochs (200000 steps)...\n",
            "Step 170000: Loss = 0.057072\n",
            "Step 170000: Loss = 0.059787\n",
            "\n",
            "Saving samples at step 170000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c22e69770231>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting training: {args.epochs} epochs ({total_steps} steps)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-c22e69770231>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    797\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviz_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'original.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownsampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'downsampled.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirect_recons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'direct_recons.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m                 \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mviz_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'upsampled.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2574\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import math\n",
        "import copy\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "from torch.utils import data\n",
        "from pathlib import Path\n",
        "from torch.optim import Adam\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "import pytorch_fid\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from pytorch_fid import fid_score\n",
        "\n",
        "# colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "res_dir = '/content/drive/MyDrive/Project'\n",
        "\n",
        "# helper funcs\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "def cycle(dl):\n",
        "    while True:\n",
        "        for data in dl:\n",
        "            yield data\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    grps = num // divisor\n",
        "    leftover = num % divisor\n",
        "    arr = [divisor] * grps\n",
        "    if leftover > 0:\n",
        "        arr.append(leftover)\n",
        "    return arr\n",
        "\n",
        "# small helpers\n",
        "class EMA():\n",
        "    def __init__(self, beta):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "\n",
        "    def update_model_average(self, ma_model, current_model):\n",
        "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
        "            old, new = ma_params.data, current_params.data\n",
        "            ma_params.data = self.update_average(old, new)\n",
        "\n",
        "    def update_average(self, old, new):\n",
        "        if old is None:\n",
        "            return new\n",
        "        return old * self.beta + (1 - self.beta) * new\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        dev = x.device\n",
        "        half_d = self.dim // 2\n",
        "        emb = math.log(10000) / (half_d - 1)\n",
        "        emb = torch.exp(torch.arange(half_d, device=dev) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
        "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n",
        "        mean = torch.mean(x, dim = 1, keepdim = True)\n",
        "        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "class ConvNextBlock(nn.Module):\n",
        "    # from that paper\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim = None, mult = 2, norm = True):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.GELU(),\n",
        "            nn.Linear(time_emb_dim, dim)\n",
        "        ) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding = 3, groups = dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            LayerNorm(dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding = 1),\n",
        "            nn.GELU(),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding = 1)\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb = None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp):\n",
        "            assert exists(time_emb), 'time emb must be passed in'\n",
        "            cond = self.mlp(time_emb)\n",
        "            h = h + rearrange(cond, 'b c -> b c 1 1')\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads = 4, dim_head = 32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
        "        q = q * self.scale\n",
        "\n",
        "        k = k.softmax(dim = -1)\n",
        "        ctx = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
        "\n",
        "        out = torch.einsum('b h d e, b h d n -> b h e n', ctx, q)\n",
        "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels = 3,\n",
        "        with_time_emb = True,\n",
        "        residual = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.residual = residual\n",
        "        print(\"Is Time embed used ? \", with_time_emb)\n",
        "\n",
        "        dims = [channels, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        if with_time_emb:\n",
        "            time_dim = dim\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPosEmb(dim),\n",
        "                nn.Linear(dim, dim * 4),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim * 4, dim)\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_in, dim_out, time_emb_dim = time_dim, norm = ind != 0),\n",
        "                ConvNextBlock(dim_out, dim_out, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                Downsample(dim_out) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
        "        self.mid_block2 = ConvNextBlock(mid_dim, mid_dim, time_emb_dim = time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ConvNextBlock(dim_out * 2, dim_in, time_emb_dim = time_dim),\n",
        "                ConvNextBlock(dim_in, dim_in, time_emb_dim = time_dim),\n",
        "                Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                Upsample(dim_in) if not is_last else nn.Identity()\n",
        "            ]))\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            ConvNextBlock(dim, dim),\n",
        "            nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        orig_x = x\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for convnext, convnext2, attn, downsample in self.downs:\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for convnext, convnext2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = convnext(x, t)\n",
        "            x = convnext2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        if self.residual:\n",
        "            return self.final_conv(x) + orig_x\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "class SuperResolutionDiffusion(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        denoise_fn,\n",
        "        *,\n",
        "        image_size,\n",
        "        device_of_kernel,\n",
        "        channels = 3,\n",
        "        timesteps = 3,\n",
        "        loss_type = 'l1',\n",
        "        train_routine = 'Final',\n",
        "        sampling_routine='x0_step_down'\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.image_size = image_size\n",
        "        self.denoise_fn = denoise_fn\n",
        "        self.device_of_kernel = device_of_kernel\n",
        "\n",
        "        self.num_timesteps = int(timesteps)\n",
        "        self.loss_type = loss_type\n",
        "        self.train_routine = train_routine\n",
        "        self.sampling_routine = sampling_routine\n",
        "\n",
        "        # need steps for downsamping\n",
        "        self.min_size = 4\n",
        "        self.num_downsamples = int(math.log2(image_size // self.min_size))\n",
        "\n",
        "        # scales for timestep\n",
        "        self.scale_factors = self.get_scale_factors()\n",
        "\n",
        "        print(f\"Super Resolution Diffusion: Image size: {image_size}, Min size: {self.min_size}\")\n",
        "        print(f\"Number of downsampling steps: {self.num_downsamples}\")\n",
        "        print(f\"Scale factors: {self.scale_factors}\")\n",
        "\n",
        "    def get_scale_factors(self):\n",
        "        \"\"\"make scale schedule\"\"\"\n",
        "        factors = []\n",
        "        for t in range(self.num_timesteps):\n",
        "            if t == 0:\n",
        "                # mild downsample first\n",
        "                scale = 1.0 / 2.0\n",
        "            else:\n",
        "                # more aggressive later\n",
        "                scale = 1.0 / (2.0 * (t + 1))\n",
        "\n",
        "            factors.append(scale)\n",
        "        return factors\n",
        "\n",
        "    def q_sample(self, x_start, t):\n",
        "        \"\"\"downsample based on t\"\"\"\n",
        "        if isinstance(t, int):\n",
        "            return self.downsample_step(x_start, t)\n",
        "\n",
        "        results = []\n",
        "        for i, timestep in enumerate(t):\n",
        "            results.append(self.downsample_step(x_start[i:i+1], timestep))\n",
        "\n",
        "        return torch.cat(results, dim=0)\n",
        "\n",
        "    def downsample_step(self, x, t):\n",
        "        \"\"\"downsample for timestep t\"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        # get scale\n",
        "        if t >= len(self.scale_factors):\n",
        "            t = len(self.scale_factors) - 1\n",
        "\n",
        "        scale = self.scale_factors[t]\n",
        "\n",
        "        # target size\n",
        "        tgt_h = max(int(h * scale), 1)\n",
        "        tgt_w = max(int(w * scale), 1)\n",
        "\n",
        "        # downsample\n",
        "        down = F.interpolate(x, size=(tgt_h, tgt_w), mode='nearest')\n",
        "\n",
        "        # back to original size\n",
        "        up = F.interpolate(down, size=(h, w), mode='nearest')\n",
        "\n",
        "        return up\n",
        "\n",
        "    def p_losses(self, x_start, t):\n",
        "        \"\"\"training loss calc\"\"\"\n",
        "        x_noisy = self.q_sample(x_start=x_start, t=t)\n",
        "        x_recon = self.denoise_fn(x_noisy, t)\n",
        "\n",
        "        if self.loss_type == 'l1':\n",
        "            loss = (x_start - x_recon).abs().mean()\n",
        "        elif self.loss_type == 'l2':\n",
        "            loss = F.mse_loss(x_start, x_recon)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        \"\"\"train forward\"\"\"\n",
        "        b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n",
        "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
        "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
        "        return self.p_losses(x, t, *args, **kwargs)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, batch_size=16, img=None, t=None):\n",
        "        \"\"\"sample using alg 2\"\"\"\n",
        "        self.denoise_fn.eval()\n",
        "\n",
        "        if t is None:\n",
        "            t = self.num_timesteps\n",
        "\n",
        "        # downsample first\n",
        "        xt = self.downsample_step(img, t-1)\n",
        "        direct_recons = None\n",
        "\n",
        "        # iterative restore\n",
        "        curr_img = xt.clone()\n",
        "\n",
        "        while(t):\n",
        "            step = torch.full((batch_size,), t - 1, dtype=torch.long).to(img.device)\n",
        "            x0_pred = self.denoise_fn(curr_img, step)  # predict clean\n",
        "\n",
        "            if direct_recons is None:\n",
        "                direct_recons = x0_pred.clone()\n",
        "\n",
        "            if self.sampling_routine == 'x0_step_down':\n",
        "                # alg 2\n",
        "                x_t = self.downsample_step(x0_pred, t-1)\n",
        "                x_t_minus_1 = self.downsample_step(x0_pred, t-2) if t > 1 else x0_pred\n",
        "\n",
        "                # key step from paper\n",
        "                curr_img = curr_img - x_t + x_t_minus_1\n",
        "\n",
        "            elif self.sampling_routine == 'default':\n",
        "                # alg 1\n",
        "                if t > 1:\n",
        "                    curr_img = self.downsample_step(x0_pred, t-2)\n",
        "                else:\n",
        "                    curr_img = x0_pred\n",
        "\n",
        "            t = t - 1\n",
        "\n",
        "        self.denoise_fn.train()\n",
        "        return xt, direct_recons, curr_img\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, folder, image_size, exts=['jpg', 'jpeg', 'png']):\n",
        "        super().__init__()\n",
        "        self.folder = folder\n",
        "        self.image_size = image_size\n",
        "        self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((int(image_size*1.12), int(image_size*1.12))),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)  # Scale to [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = Image.open(path)\n",
        "        return self.transform(img)\n",
        "\n",
        "def calculate_metrics(orig_imgs, recon_imgs):\n",
        "    orig_np = (orig_imgs.cpu().permute(0, 2, 3, 1) + 1) / 2\n",
        "    recon_np = (recon_imgs.cpu().permute(0, 2, 3, 1) + 1) / 2\n",
        "\n",
        "    # ssim\n",
        "    ssim_vals = []\n",
        "    for i in range(orig_np.shape[0]):\n",
        "        orig = orig_np[i].numpy()\n",
        "        recon = recon_np[i].numpy()\n",
        "\n",
        "        if orig.shape[2] == 1:\n",
        "            orig = orig.squeeze(2)\n",
        "            recon = recon.squeeze(2)\n",
        "            ssim_val = ssim(orig, recon, data_range=1.0, win_size=5, channel_axis=None)\n",
        "        else:\n",
        "            ssim_val = ssim(orig, recon, data_range=1.0, win_size=5, channel_axis=2)\n",
        "\n",
        "        ssim_vals.append(ssim_val)\n",
        "\n",
        "    avg_ssim = np.mean(ssim_vals)\n",
        "\n",
        "    # rmse\n",
        "    mse = F.mse_loss(orig_imgs, recon_imgs).item()\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # fid stuff\n",
        "    tmp_orig = 'temp_orig'\n",
        "    tmp_recon = 'temp_recon'\n",
        "    os.makedirs(tmp_orig, exist_ok=True)\n",
        "    os.makedirs(tmp_recon, exist_ok=True)\n",
        "\n",
        "    # save images for fid\n",
        "    for i in range(min(orig_imgs.shape[0], 100)):\n",
        "        utils.save_image((orig_imgs[i] + 1) / 2, os.path.join(tmp_orig, f'{i}.png'))\n",
        "        utils.save_image((recon_imgs[i] + 1) / 2, os.path.join(tmp_recon, f'{i}.png'))\n",
        "\n",
        "    # calc fid\n",
        "    fid = fid_score.calculate_fid_given_paths([tmp_orig, tmp_recon],\n",
        "                                   batch_size=50, device=orig_imgs.device,\n",
        "                                   dims=2048)\n",
        "    # cleanup\n",
        "    shutil.rmtree(tmp_orig)\n",
        "    shutil.rmtree(tmp_recon)\n",
        "\n",
        "    return avg_ssim, rmse, fid\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        diffusion_model,\n",
        "        dataset_type,\n",
        "        *,\n",
        "        ema_decay = 0.995,\n",
        "        image_size = 32,\n",
        "        train_batch_size = 32,\n",
        "        eval_batch_size = 16,\n",
        "        train_lr = 2e-5,\n",
        "        train_num_steps = 100000,\n",
        "        gradient_accumulate_every = 2,\n",
        "        step_start_ema = 2000,\n",
        "        update_ema_every = 10,\n",
        "        save_and_sample_every = 1000,\n",
        "        results_folder = './results',\n",
        "        load_path = None,\n",
        "        eval_dataset = None,\n",
        "        save_model_every = 10000,\n",
        "        eval_every = 10000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = diffusion_model\n",
        "        self.ema = EMA(ema_decay)\n",
        "        self.ema_model = copy.deepcopy(self.model)\n",
        "        self.update_ema_every = update_ema_every\n",
        "\n",
        "        self.step_start_ema = step_start_ema\n",
        "        self.save_and_sample_every = save_and_sample_every\n",
        "        self.save_model_every = save_model_every\n",
        "        self.eval_every = eval_every\n",
        "\n",
        "        self.batch_size = train_batch_size\n",
        "        self.eval_batch_size = eval_batch_size\n",
        "        self.image_size = image_size\n",
        "        self.gradient_accumulate_every = gradient_accumulate_every\n",
        "        self.train_num_steps = train_num_steps\n",
        "\n",
        "        self.dataset_type = dataset_type\n",
        "        self.setup_dataset(dataset_type)\n",
        "\n",
        "        self.eval_dataset = eval_dataset\n",
        "        if eval_dataset is None:\n",
        "            # use training subset\n",
        "            eval_size = min(1000, len(self.ds))\n",
        "            eval_indices = np.random.choice(len(self.ds), eval_size, replace=False)\n",
        "            self.eval_dataset = torch.utils.data.Subset(self.ds, eval_indices)\n",
        "\n",
        "        self.eval_dl = data.DataLoader(self.eval_dataset, batch_size=eval_batch_size,\n",
        "                                    shuffle=False, drop_last=False)\n",
        "\n",
        "        self.opt = Adam(diffusion_model.parameters(), lr=train_lr)\n",
        "        self.step = 0\n",
        "\n",
        "        self.results_folder = Path(results_folder)\n",
        "        self.results_folder.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "        # metrics file\n",
        "        self.metrics_file = self.results_folder / 'metrics.csv'\n",
        "        if not self.metrics_file.exists():\n",
        "            with open(self.metrics_file, 'w') as f:\n",
        "                f.write('step,ssim,rmse,fid\\n')\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        if load_path is not None:\n",
        "            self.load(load_path)\n",
        "\n",
        "    def setup_dataset(self, dataset_type):\n",
        "        if dataset_type == 'mnist':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.Pad(2),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "            ])\n",
        "\n",
        "            # get mnist\n",
        "            self.ds = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=self.batch_size, shuffle=True,\n",
        "                                        pin_memory=True, num_workers=8, drop_last=True))\n",
        "\n",
        "        elif dataset_type == 'cifar10':\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "            ])\n",
        "\n",
        "            # get cifar\n",
        "            self.ds = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "            self.dl = cycle(data.DataLoader(self.ds, batch_size=self.batch_size, shuffle=True,\n",
        "                                        pin_memory=True, num_workers=8, drop_last=True))\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.ema_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def step_ema(self):\n",
        "        if self.step < self.step_start_ema:\n",
        "            self.reset_parameters()\n",
        "            return\n",
        "        self.ema.update_model_average(self.ema_model, self.model)\n",
        "\n",
        "    def save(self, milestone=None):\n",
        "        data = {\n",
        "            'step': self.step,\n",
        "            'model': self.model.state_dict(),\n",
        "            'ema': self.ema_model.state_dict()\n",
        "        }\n",
        "        if milestone is None:\n",
        "            torch.save(data, str(self.results_folder / f'model.pt'))\n",
        "        else:\n",
        "            torch.save(data, str(self.results_folder / f'model_{milestone}.pt'))\n",
        "\n",
        "    def load(self, load_path):\n",
        "        print(f\"Loading model from {load_path}\")\n",
        "        data = torch.load(load_path)\n",
        "\n",
        "        self.step = data['step']\n",
        "        self.model.load_state_dict(data['model'])\n",
        "        self.ema_model.load_state_dict(data['ema'])\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"eval on test set\"\"\"\n",
        "        self.ema_model.eval()\n",
        "\n",
        "        all_orig_imgs = []\n",
        "        all_downsampled_imgs = []\n",
        "        all_direct_recons = []\n",
        "        all_upsampled_imgs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in self.eval_dl:\n",
        "                if isinstance(batch, (list, tuple)):\n",
        "                    # handle (img, label)\n",
        "                    orig_imgs = batch[0].to(self.model.device_of_kernel)\n",
        "                else:\n",
        "                    # handle just img\n",
        "                    orig_imgs = batch.to(self.model.device_of_kernel)\n",
        "\n",
        "                # use model for SR\n",
        "                downsampled_imgs, direct_recons, upsampled_imgs = self.ema_model.sample(\n",
        "                    batch_size=orig_imgs.shape[0], img=orig_imgs)\n",
        "\n",
        "                all_orig_imgs.append(orig_imgs)\n",
        "                all_downsampled_imgs.append(downsampled_imgs)\n",
        "                all_direct_recons.append(direct_recons)\n",
        "                all_upsampled_imgs.append(upsampled_imgs)\n",
        "\n",
        "                # limit to 100 imgs\n",
        "                if len(all_orig_imgs) * self.eval_batch_size >= 100:\n",
        "                    break\n",
        "\n",
        "        # cat everything\n",
        "        orig_imgs = torch.cat(all_orig_imgs, dim=0)\n",
        "        downsampled_imgs = torch.cat(all_downsampled_imgs, dim=0)\n",
        "        direct_recons = torch.cat(all_direct_recons, dim=0)\n",
        "        upsampled_imgs = torch.cat(all_upsampled_imgs, dim=0)\n",
        "\n",
        "        # direct recon metrics\n",
        "        direct_ssim, direct_rmse, direct_fid = calculate_metrics(orig_imgs, direct_recons)\n",
        "\n",
        "        # alg 2 metrics\n",
        "        upsampled_ssim, upsampled_rmse, upsampled_fid = calculate_metrics(orig_imgs, upsampled_imgs)\n",
        "\n",
        "        # baseline metrics\n",
        "        downsampled_ssim, downsampled_rmse, downsampled_fid = calculate_metrics(orig_imgs, downsampled_imgs)\n",
        "\n",
        "        # results dict\n",
        "        metrics = {\n",
        "            'direct': {'ssim': direct_ssim, 'rmse': direct_rmse, 'fid': direct_fid},\n",
        "            'upsampled': {'ssim': upsampled_ssim, 'rmse': upsampled_rmse, 'fid': upsampled_fid},\n",
        "            'downsampled': {'ssim': downsampled_ssim, 'rmse': downsampled_rmse, 'fid': downsampled_fid}\n",
        "        }\n",
        "\n",
        "        # log to file\n",
        "        with open(self.metrics_file, 'a') as f:\n",
        "            f.write(f\"{self.step},{upsampled_ssim},{upsampled_rmse},{upsampled_fid}\\n\")\n",
        "\n",
        "        # make pictures\n",
        "        vis_folder = self.results_folder / f\"eval_{self.step}\"\n",
        "        vis_folder.mkdir(exist_ok=True)\n",
        "\n",
        "        # save imgs\n",
        "        n_samples = min(8, orig_imgs.shape[0])\n",
        "        samples = torch.stack([\n",
        "            orig_imgs[:n_samples],\n",
        "            downsampled_imgs[:n_samples],\n",
        "            direct_recons[:n_samples],\n",
        "            upsampled_imgs[:n_samples]\n",
        "        ])\n",
        "        samples = (samples + 1) / 2  # [0, 1] range\n",
        "\n",
        "        # grid: diff samples in rows\n",
        "        grid = utils.make_grid(samples.view(-1, *samples.shape[2:]), nrow=n_samples)\n",
        "        utils.save_image(grid, vis_folder / 'samples.png')\n",
        "\n",
        "        # plot metrics\n",
        "        if self.step > 0:\n",
        "            self.plot_metrics()\n",
        "\n",
        "        print(f\"Eval at step {self.step}:\")\n",
        "        print(f\"Direct recon: SSIM={direct_ssim:.4f}, RMSE={direct_rmse:.4f}, FID={direct_fid:.2f}\")\n",
        "        print(f\"Super-Res (Alg 2): SSIM={upsampled_ssim:.4f}, RMSE={upsampled_rmse:.4f}, FID={upsampled_fid:.2f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"plot stuff over time\"\"\"\n",
        "        import pandas as pd\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        df = pd.read_csv(self.metrics_file)\n",
        "\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # ssim plot\n",
        "        axs[0].plot(df['step'], df['ssim'])\n",
        "        axs[0].set_title('SSIM over time (higher=better)')\n",
        "        axs[0].set_xlabel('Training steps')\n",
        "        axs[0].set_ylabel('SSIM')\n",
        "\n",
        "        # rmse plot\n",
        "        axs[1].plot(df['step'], df['rmse'])\n",
        "        axs[1].set_title('RMSE over time (lower=better)')\n",
        "        axs[1].set_xlabel('Training steps')\n",
        "        axs[1].set_ylabel('RMSE')\n",
        "\n",
        "        # fid plot\n",
        "        axs[2].plot(df['step'], df['fid'])\n",
        "        axs[2].set_title('FID over time (lower=better)')\n",
        "        axs[2].set_xlabel('Training steps')\n",
        "        axs[2].set_ylabel('FID')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.results_folder / 'metrics_plot.png')\n",
        "        plt.close()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"train the model\"\"\"\n",
        "        device = self.model.device_of_kernel\n",
        "\n",
        "        acc_loss = 0\n",
        "        while self.step < self.train_num_steps:\n",
        "            uloss = 0\n",
        "            for i in range(self.gradient_accumulate_every):\n",
        "                # get batch\n",
        "                if self.dataset_type in ['mnist', 'cifar10']:\n",
        "                    # handle (img, label)\n",
        "                    data, _ = next(self.dl)\n",
        "                    data = data.to(device)\n",
        "                else:\n",
        "                    # handle just imgs\n",
        "                    data = next(self.dl).to(device)\n",
        "\n",
        "                # forward\n",
        "                loss = torch.mean(self.model(data))\n",
        "                if self.step % 100 == 0:\n",
        "                    print(f'Step {self.step}: Loss = {loss.item():.6f}')\n",
        "\n",
        "                uloss += loss.item()\n",
        "\n",
        "                # backward with grad accum\n",
        "                loss = loss / self.gradient_accumulate_every\n",
        "                loss.backward()\n",
        "\n",
        "            # update\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "            # track avg loss\n",
        "            acc_loss = acc_loss + (uloss / self.gradient_accumulate_every)\n",
        "\n",
        "            # ema update\n",
        "            if self.step % self.update_ema_every == 0:\n",
        "                self.step_ema()\n",
        "\n",
        "            # save stuff and metrics\n",
        "            if self.step != 0 and self.step % self.save_and_sample_every == 0:\n",
        "                milestone = self.step // self.save_and_sample_every\n",
        "                print(f\"\\nSaving samples at step {self.step}\")\n",
        "\n",
        "                # get imgs for viz\n",
        "                if self.dataset_type in ['mnist', 'cifar10']:\n",
        "                    viz_batch, _ = next(self.dl)\n",
        "                    viz_batch = viz_batch.to(device)\n",
        "                else:\n",
        "                    viz_batch = next(self.dl).to(device)\n",
        "\n",
        "                # sample w/ ema\n",
        "                with torch.no_grad():\n",
        "                    downsampled, direct_recons, upsampled = self.ema_model.sample(\n",
        "                        batch_size=viz_batch.shape[0], img=viz_batch)\n",
        "\n",
        "                # save imgs\n",
        "                viz_folder = self.results_folder / f\"samples_{self.step}\"\n",
        "                viz_folder.mkdir(exist_ok=True)\n",
        "\n",
        "                # [0,1] range for viz\n",
        "                viz_batch = (viz_batch + 1) * 0.5\n",
        "                downsampled = (downsampled + 1) * 0.5\n",
        "                direct_recons = (direct_recons + 1) * 0.5\n",
        "                upsampled = (upsampled + 1) * 0.5\n",
        "\n",
        "                # grid + save\n",
        "                grid = torch.cat([\n",
        "                    viz_batch[:8],\n",
        "                    downsampled[:8],\n",
        "                    direct_recons[:8],\n",
        "                    upsampled[:8]\n",
        "                ])\n",
        "                utils.save_image(grid, viz_folder / 'grid.png', nrow=8)\n",
        "\n",
        "                # individual imgs\n",
        "                utils.save_image(viz_batch[:8], viz_folder / 'original.png', nrow=4)\n",
        "                utils.save_image(downsampled[:8], viz_folder / 'downsampled.png', nrow=4)\n",
        "                utils.save_image(direct_recons[:8], viz_folder / 'direct_recons.png', nrow=4)\n",
        "                utils.save_image(upsampled[:8], viz_folder / 'upsampled.png', nrow=4)\n",
        "\n",
        "                # print avg loss\n",
        "                avg_loss = acc_loss / (self.save_and_sample_every)\n",
        "                print(f'Avg loss (last {self.save_and_sample_every} steps): {avg_loss:.6f}')\n",
        "                acc_loss = 0\n",
        "\n",
        "            # save checkpoint\n",
        "            if self.step != 0 and self.step % self.save_model_every == 0:\n",
        "                self.save(self.step)\n",
        "                print(f\"Model saved at step {self.step}\")\n",
        "\n",
        "            # eval model\n",
        "            if self.step != 0 and self.step % self.eval_every == 0:\n",
        "                print(f\"\\nEvaluating at step {self.step}\")\n",
        "                self.evaluate()\n",
        "\n",
        "            self.step += 1\n",
        "\n",
        "        # final stuff\n",
        "        self.save()\n",
        "        self.evaluate()\n",
        "        print('Training done!')\n",
        "\n",
        "# Run main\n",
        "args = type('Args', (), {\n",
        "    'dataset': 'cifar10',  # mnist or cifar10\n",
        "    'batch_size': 32,\n",
        "    'eval_batch_size': 16,\n",
        "    'epochs': 50,\n",
        "    'save_every': 10000,\n",
        "    'eval_every': 10000,\n",
        "    'sample_every': 1000,\n",
        "    'results_dir': f\"{res_dir}\",\n",
        "    'load_model': '/content/drive/MyDrive/Project/super_cifar10/model_170000.pt',\n",
        "    'train': True,\n",
        "    'eval': True\n",
        "})()\n",
        "\n",
        "# device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# setup config\n",
        "if args.dataset == 'mnist':\n",
        "    channels = 1\n",
        "    image_size = 32\n",
        "    model_dim = 64\n",
        "    timesteps = 3\n",
        "elif args.dataset == 'cifar10':\n",
        "    channels = 3\n",
        "    image_size = 32\n",
        "    model_dim = 64\n",
        "    timesteps = 3\n",
        "\n",
        "# make results dir\n",
        "results_dir = Path(args.results_dir) / f\"super_{args.dataset}\"\n",
        "results_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# make model\n",
        "model = Unet(\n",
        "    dim=model_dim,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4, 8),\n",
        "    with_time_emb=True\n",
        ").to(device)\n",
        "\n",
        "# make diffusion\n",
        "diffusion = SuperResolutionDiffusion(\n",
        "    model,\n",
        "    image_size=image_size,\n",
        "    device_of_kernel=device,\n",
        "    channels=channels,\n",
        "    timesteps=timesteps,\n",
        "    loss_type='l1',\n",
        "    train_routine='Final',\n",
        "    sampling_routine='x0_step_down'  # use alg 2\n",
        ").to(device)\n",
        "\n",
        "total_steps = 200000\n",
        "\n",
        "# make trainer\n",
        "trainer = Trainer(\n",
        "    diffusion,\n",
        "    args.dataset,\n",
        "    image_size=image_size,\n",
        "    train_batch_size=args.batch_size,\n",
        "    eval_batch_size=args.eval_batch_size,\n",
        "    train_lr=2e-5,\n",
        "    train_num_steps=total_steps,\n",
        "    save_and_sample_every=args.sample_every,\n",
        "    save_model_every=args.save_every,\n",
        "    eval_every=args.eval_every,\n",
        "    results_folder=results_dir,\n",
        "    load_path=args.load_model\n",
        ")\n",
        "\n",
        "if args.train:\n",
        "    print(f\"Starting training: {args.epochs} epochs ({total_steps} steps)...\")\n",
        "    trainer.train()\n",
        "\n",
        "if args.eval:\n",
        "    print(\"Evaluating model...\")\n",
        "    trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}